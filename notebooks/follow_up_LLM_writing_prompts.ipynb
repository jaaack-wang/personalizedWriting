{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8748277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19112a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86c0510",
   "metadata": {},
   "source": [
    "### Setting 2\n",
    "\n",
    "\n",
    "For each sample in the evaluation set, provide LLMs with 5 samples from the training data that belong to the particular cluster of the evaluation sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096448d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scripts.utils import count_words, round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting1\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting2(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    num_exemplars=5):\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= num_exemplars, \\\n",
    "        f\"Each author must have at least {num_exemplars} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "    \n",
    "    assert \"cluster\" in training_df.columns, \\\n",
    "        f\"Cluster column 'cluster' not found in training DataFrame.\"\n",
    "    \n",
    "    assert \"cluster\" in evaluation_df.columns, \\\n",
    "        f\"Cluster column 'cluster' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy()\n",
    "    prompt_tmp = get_prompt_template_for_writing_setting1()        \n",
    "    \n",
    "    print(f\"Generating prompts...\")\n",
    "    for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "        \n",
    "        author = row[author_col]\n",
    "        summary = row[summary_col]\n",
    "        cluster = row[\"cluster\"]\n",
    "        \n",
    "        num_words = round_up_to_nearest_10(count_words(row[text_col]))\n",
    "        samples = training_df[(training_df[author_col]==author) & \n",
    "                              (training_df[\"cluster\"]==cluster)][text_col].sample(num_exemplars)\n",
    "        \n",
    "        writing_samples = list_writing_samples(samples)\n",
    "        prompt = prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                       genre=genre, num_words=num_words,\n",
    "                                       summary=summary)\n",
    "        evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "        evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "    # evaluation_df.to_csv(evaluation_df_fp, index=False)\n",
    "    \n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39bc8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        evaluation_df = create_writing_prompts_setting2(\n",
    "            training_df_fp=f\"dataset_followup/{dataset}_train.csv\",\n",
    "            evaluation_df_fp=f\"dataset_followup/{dataset}_test.csv\",\n",
    "            genre=\"blog\",\n",
    "            author_col=\"author\", \n",
    "            text_col=\"text\", \n",
    "            summary_col=\"summary\", \n",
    "            num_exemplars=5\n",
    "        )\n",
    "        train = pd.read_csv(f\"dataset_followup/{dataset}_train.csv\")\n",
    "\n",
    "        for ix, row in evaluation_df.iterrows():\n",
    "            ixes = [int(i) for i in row[\"training sample indices\"].split(\",\")]\n",
    "            cluster = row[\"cluster\"]\n",
    "\n",
    "            cluster_train = train[train[\"cluster\"]==cluster][\"cluster\"].to_list()\n",
    "            assert len(set(cluster_train)) == 1, \\\n",
    "                f\"Cluster mismatch for author {row['author']} in dataset {dataset}. \" \\\n",
    "                f\"Expected cluster: {cluster}, Found clusters: {set(cluster_train)}\"\n",
    "            assert cluster_train[0] == cluster, \\\n",
    "                f\"Cluster mismatch for author {row['author']} in dataset {dataset}. \" \\\n",
    "                f\"Expected cluster: {cluster}, Found clusters: {set(cluster_train)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a5ea433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 568.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 785.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 674.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 828.41it/s]\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7182765",
   "metadata": {},
   "source": [
    "### Setting 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scripts.utils import count_words, round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting1\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting3(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    num_exemplars=5):\n",
    "    '''Create writing prompts for the evaluation set based on the training set.\n",
    "    For each sample in the evaluation set, find the num_exemplars most similar samples\n",
    "    in the training set based on word count. The prompt will include the writing samples\n",
    "    and the summary of the evaluation sample.\n",
    "    '''\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "    training_df[\"num_words\"] = training_df[text_col].apply(count_words)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= num_exemplars, \\\n",
    "        f\"Each author must have at least {num_exemplars} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy()\n",
    "    prompt_tmp = get_prompt_template_for_writing_setting1()        \n",
    "    \n",
    "    print(f\"Generating prompts...\")\n",
    "    for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "        \n",
    "        author = row[author_col]\n",
    "        summary = row[summary_col]\n",
    "        \n",
    "        num_words = count_words(row[text_col])\n",
    "        samples = training_df.copy()[training_df[author_col]==author]\n",
    "        samples[\"wc_diff\"] = abs(samples[\"num_words\"] - num_words)\n",
    "        samples = samples.sort_values(\"wc_diff\", ).head(num_exemplars)\n",
    "        \n",
    "        writing_samples = list_writing_samples(samples)\n",
    "        prompt = prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                       genre=genre, num_words=round_up_to_nearest_10(num_words),\n",
    "                                       summary=summary)\n",
    "        evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "        evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "    \n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332469a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e628fcbd",
   "metadata": {},
   "source": [
    "### Setting 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "944da502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scripts.utils import round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting5\n",
    "\n",
    "\n",
    "def align_df1_to_df2(df1, df2, col1, col2, col3):\n",
    "    \"\"\"\n",
    "    Aligns df1 to df2 based on matching values in columns `col1,` `col2` and `col3`,\n",
    "    handling duplicates in both dataframes.\n",
    "\n",
    "    Returns a new DataFrame with the same number of rows and order as df2,\n",
    "    by selecting matching rows from df1.\n",
    "    \"\"\"\n",
    "    df1_temp = df1.copy()\n",
    "    df1_temp['_used'] = False\n",
    "    matched_rows = []\n",
    "\n",
    "    for _, row in df2.iterrows():\n",
    "        match = df1_temp[\n",
    "            (df1_temp[col1] == row[col1]) &\n",
    "            (df1_temp[col2] == row[col2]) &\n",
    "            (df1_temp[col3] == row[col3]) &\n",
    "            (~df1_temp['_used'])\n",
    "        ]\n",
    "\n",
    "        if match.empty:\n",
    "            return None\n",
    "\n",
    "        first_match_idx = match.index[0]\n",
    "        matched_rows.append(df1_temp.loc[first_match_idx])\n",
    "        df1_temp.at[first_match_idx, '_used'] = True\n",
    "\n",
    "    result = pd.DataFrame(matched_rows).drop(columns=['_used']).reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting5(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    num_exemplars=5):\n",
    "    \n",
    "    def get_text_snippet(text, percentage=0.2):\n",
    "        words = word_tokenize(text)\n",
    "        num_words = len(words)\n",
    "        snippet_length = min(50, int(num_words * percentage))\n",
    "        length_to_continue = num_words - snippet_length\n",
    "        snippet = \" \".join(words[:snippet_length])\n",
    "        return snippet, length_to_continue\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= num_exemplars, \\\n",
    "        f\"Each author must have at least {num_exemplars} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy().reset_index(drop=True)\n",
    "    summary_only_prompt_tmp, exemplars_plus_summary_prompt_tmp = \\\n",
    "        get_prompt_template_for_writing_setting5()\n",
    "\n",
    "    print(f\"Generating summary-only prompts...\")\n",
    "    for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "        summary = row[summary_col]\n",
    "        snippet, length_to_continue = get_text_snippet(row[text_col])\n",
    "        length_to_continue = round_up_to_nearest_10(length_to_continue)\n",
    "        prompt = summary_only_prompt_tmp.substitute(genre=genre, \n",
    "                                                    num_words=length_to_continue, \n",
    "                                                    summary=summary, \n",
    "                                                    snippet=snippet)\n",
    "        evaluation_df.at[ix, \"training sample indices\"] = \"-\"\n",
    "        evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "    evaluation_df[\"Condition\"] = \"summary-only\"\n",
    "    out = [evaluation_df.copy()]\n",
    "    print(f\"Generating exemplars-plus-summary prompts...\")\n",
    "    \n",
    "    dataset = evaluation_df_fp.split(\"/\")[-1].split(\"_\")[0]\n",
    "    earlier_setting1_prompts_df = None\n",
    "    earlier_setting1_prompts_fp = f\"LLM_writing/Setting1/{dataset}/prompts.csv\"\n",
    "    \n",
    "    if os.path.exists(earlier_setting1_prompts_fp):\n",
    "        earlier_setting1_prompts_df = pd.read_csv(earlier_setting1_prompts_fp)\n",
    "        earlier_setting1_prompts_df = align_df1_to_df2(earlier_setting1_prompts_df, evaluation_df, \n",
    "                                                       text_col, author_col, summary_col)\n",
    "        use_random_samples = True if earlier_setting1_prompts_df is None else False\n",
    "    \n",
    "    for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "        author = row[author_col]\n",
    "        summary = row[summary_col]\n",
    "        snippet, length_to_continue = get_text_snippet(row[text_col])\n",
    "        length_to_continue = round_up_to_nearest_10(length_to_continue)\n",
    "\n",
    "        if earlier_setting1_prompts_df is not None and not use_random_samples:\n",
    "\n",
    "            indices = [int(i) for i in earlier_setting1_prompts_df.at[ix, \"training sample indices\"].split(\",\")]\n",
    "            samples = training_df.loc[indices]\n",
    "\n",
    "            if len(samples) != num_exemplars:\n",
    "                print(f\"Number of samples found for {row[text_col]} in earlier setting1 prompts is not {num_exemplars}. \")\n",
    "                use_random_samples = True\n",
    "                break\n",
    "                \n",
    "            if len(samples[author_col].unique()) != 1:\n",
    "                print(f\"More than one author found for sample {row[text_col]} in earlier setting1 prompts. \" \\\n",
    "                      f\"Found authors: {samples[author_col].unique()}\")\n",
    "                use_random_samples = True\n",
    "                break\n",
    "\n",
    "            if samples[author_col].values[0] != author:\n",
    "                print(f\"Author mismatch for sample {row[text_col]} in earlier setting1 prompts.\")\n",
    "                use_random_samples = True\n",
    "                break\n",
    "\n",
    "            samples = samples[text_col]\n",
    "        \n",
    "        if use_random_samples:\n",
    "            samples = training_df[training_df[author_col]==author][text_col].sample(num_exemplars)\n",
    "\n",
    "        writing_samples = list_writing_samples(samples)\n",
    "        prompt = exemplars_plus_summary_prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                                              genre=genre, \n",
    "                                                              num_words=length_to_continue,\n",
    "                                                              summary=summary, \n",
    "                                                              snippet=snippet)\n",
    "        \n",
    "        evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "        evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "    \n",
    "    evaluation_df[\"Condition\"] = \"exemplars-plus-summary\"\n",
    "    \n",
    "    out.append(evaluation_df.copy())\n",
    "    out_df = pd.concat(out, ignore_index=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f70ff249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        if dataset.startswith(\"blog\"):\n",
    "            genre = \"blog post\"\n",
    "        elif dataset.startswith(\"enron\"):\n",
    "            genre = \"email\"\n",
    "        elif dataset.startswith(\"reddit\"):\n",
    "            genre = \"reddit post\"\n",
    "        elif dataset.startswith(\"CCAT50\"):\n",
    "            genre = \"news article\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset}. Please specify a genre.\")\n",
    "\n",
    "        evaluation_df = create_writing_prompts_setting5(\n",
    "            training_df_fp=f\"dataset_followup/{dataset}_train.csv\",\n",
    "            evaluation_df_fp=f\"dataset_followup/{dataset}_test.csv\",\n",
    "            genre=genre,\n",
    "            author_col=\"author\", \n",
    "            text_col=\"text\", \n",
    "            summary_col=\"summary\", \n",
    "            num_exemplars=5\n",
    "        )\n",
    "        evaluation_df = evaluation_df.copy()[evaluation_df.Condition == \"exemplars-plus-summary\"].reset_index(drop=True)\n",
    "        \n",
    "        earlier_eval_df = pd.read_csv(f\"LLM_writing/Setting1/{dataset}/prompts.csv\")\n",
    "        earlier_eval_df = align_df1_to_df2(earlier_eval_df, evaluation_df, \n",
    "                                           \"text\", \"author\", \"summary\")\n",
    "\n",
    "        assert len(earlier_eval_df) == len(evaluation_df), \\\n",
    "            f\"Number of samples in earlier setting1 prompts and new setting5 prompts do not match. \" \\\n",
    "            f\"Earlier: {len(earlier_eval_df)}, New: {len(evaluation_df)}\"\n",
    "\n",
    "        assert earlier_eval_df[\"training sample indices\"].to_list() == evaluation_df[\"training sample indices\"].to_list(), \\\n",
    "            f\"Training sample indices do not match between earlier setting1 prompts and new setting5 prompts. \" \\\n",
    "            f\"Earlier: {earlier_eval_df['training sample indices'].to_list()}, New: {evaluation_df['training sample indices'].to_list()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15e430ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary-only prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 770.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating exemplars-plus-summary prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 640.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary-only prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 1249.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating exemplars-plus-summary prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 972.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary-only prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1206.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating exemplars-plus-summary prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 946.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary-only prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1133.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating exemplars-plus-summary prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 929.46it/s]\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db1feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d351edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enron_test = pd.read_csv(\"dataset_followup/enron_test.csv\")\n",
    "blog_test = pd.read_csv(\"dataset_followup/blog_test.csv\")\n",
    "CCAT50_test = pd.read_csv(\"dataset_followup/CCAT50_test.csv\")\n",
    "reddit_test = pd.read_csv(\"dataset_followup/reddit_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aac51c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['author', 'text', 'subject', 'AA-label', 'summary', 'cluster'], dtype='object'),\n",
       " Index(['author', 'text', 'topic', 'gender', 'age', 'sign', 'date', 'AA-label',\n",
       "        'summary', 'cluster'],\n",
       "       dtype='object'),\n",
       " Index(['author', 'text', 'file_name', 'AA-label', 'summary', 'cluster'], dtype='object'),\n",
       " Index(['index', 'author', 'text', 'subreddit', 'AA-label', 'summary',\n",
       "        'cluster'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_test.columns, blog_test.columns, CCAT50_test.columns, reddit_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39a4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count     300.000000\n",
       " mean      330.516667\n",
       " std       288.417507\n",
       " min       102.000000\n",
       " 25%       148.000000\n",
       " 50%       203.000000\n",
       " 75%       397.000000\n",
       " max      1451.000000\n",
       " Name: text, dtype: float64,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.utils import count_words\n",
    "\n",
    "\n",
    "enron_test[\"text\"].apply(count_words).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680f16a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     500.000000\n",
       "mean      330.428000\n",
       "std       247.381292\n",
       "min       101.000000\n",
       "25%       156.000000\n",
       "50%       252.500000\n",
       "75%       416.000000\n",
       "max      1402.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test[\"text\"].apply(count_words).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffdd3791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     500.00000\n",
       "mean      342.25800\n",
       "std       251.85395\n",
       "min       101.00000\n",
       "25%       165.75000\n",
       "50%       265.00000\n",
       "75%       433.00000\n",
       "max      1427.00000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_test[\"text\"].apply(count_words).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1e77ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     300.000000\n",
       "mean      577.880000\n",
       "std       148.238167\n",
       "min       107.000000\n",
       "25%       490.000000\n",
       "50%       588.500000\n",
       "75%       662.500000\n",
       "max      1228.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CCAT50_test[\"text\"].apply(count_words).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42cb545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16746,6286,12122,9723,15564'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.read_csv(\"LLM_writing/Setting1/blog/prompts.csv\")\n",
    "vs = dd[\"training sample indices\"].values[0]\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad81ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16746, 6286, 12122, 9723, 15564]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b966ee11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "      <th>training sample indices</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16746</th>\n",
       "      <td>15365</td>\n",
       "      <td>Repair &amp; Maintenance &gt;&gt; Wheels &gt;&gt; Tire ...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>20,April,2004</td>\n",
       "      <td>To fix a flat bike tire: remove the wheel, tak...</td>\n",
       "      <td>6942,22531,13837,1565,10317</td>\n",
       "      <td>You will be given one or more writing samples ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6286</th>\n",
       "      <td>1000866</td>\n",
       "      <td>I would love people to just answer this...</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Libra</td>\n",
       "      <td>06,June,2003</td>\n",
       "      <td>The speaker expresses feelings of loneliness a...</td>\n",
       "      <td>2356,3102,190,5348,23591</td>\n",
       "      <td>You will be given one or more writing samples ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12122</th>\n",
       "      <td>3211137</td>\n",
       "      <td>Wow, what a weekend.  I planned a lot o...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>18,January,2004</td>\n",
       "      <td>The writer describes a busy and eventful weeke...</td>\n",
       "      <td>8920,12500,23520,6083,22325</td>\n",
       "      <td>You will be given one or more writing samples ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>2123946</td>\n",
       "      <td>Holy crap,  urlLink this  is ...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>08,February,2004</td>\n",
       "      <td>The writer describes a beautiful day with suns...</td>\n",
       "      <td>4928,25212,20297,23154,5556</td>\n",
       "      <td>You will be given one or more writing samples ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15564</th>\n",
       "      <td>180519</td>\n",
       "      <td>Well well well (deep subject) Inspired ...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>13,September,2003</td>\n",
       "      <td>The author revives their blog, Buddha Stew, af...</td>\n",
       "      <td>17096,9464,24570,24577,2874</td>\n",
       "      <td>You will be given one or more writing samples ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                               text    topic  \\\n",
       "16746    15365         Repair & Maintenance >> Wheels >> Tire ...   indUnk   \n",
       "6286   1000866         I would love people to just answer this...  Student   \n",
       "12122  3211137         Wow, what a weekend.  I planned a lot o...   indUnk   \n",
       "9723   2123946                   Holy crap,  urlLink this  is ...   indUnk   \n",
       "15564   180519         Well well well (deep subject) Inspired ...   indUnk   \n",
       "\n",
       "       gender  age         sign               date  \\\n",
       "16746  female   34       Cancer      20,April,2004   \n",
       "6286   female   17        Libra       06,June,2003   \n",
       "12122    male   16  Sagittarius    18,January,2004   \n",
       "9723   female   24    Capricorn   08,February,2004   \n",
       "15564  female   23       Cancer  13,September,2003   \n",
       "\n",
       "                                                 summary  \\\n",
       "16746  To fix a flat bike tire: remove the wheel, tak...   \n",
       "6286   The speaker expresses feelings of loneliness a...   \n",
       "12122  The writer describes a busy and eventful weeke...   \n",
       "9723   The writer describes a beautiful day with suns...   \n",
       "15564  The author revives their blog, Buddha Stew, af...   \n",
       "\n",
       "           training sample indices  \\\n",
       "16746  6942,22531,13837,1565,10317   \n",
       "6286      2356,3102,190,5348,23591   \n",
       "12122  8920,12500,23520,6083,22325   \n",
       "9723   4928,25212,20297,23154,5556   \n",
       "15564  17096,9464,24570,24577,2874   \n",
       "\n",
       "                                                  prompt  \n",
       "16746  You will be given one or more writing samples ...  \n",
       "6286   You will be given one or more writing samples ...  \n",
       "12122  You will be given one or more writing samples ...  \n",
       "9723   You will be given one or more writing samples ...  \n",
       "15564  You will be given one or more writing samples ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.loc[list(eval(vs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1974f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "910db0eb",
   "metadata": {},
   "source": [
    "### Setting 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65057324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scripts.utils import count_words, round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting1\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting6(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    nums_exemplars=[2, 4, 6, 8, 10]):\n",
    "    '''Similar to setting 1, but for each sample in the evaluation set,\n",
    "    find multiple num_exemplars random samples in the training set to be used as writing examples.\n",
    "    A larger nums_exemplar will subsume all smaller nums_exemplar. \n",
    "    '''\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "    nums_exemplars = sorted(nums_exemplars, reverse=True)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= nums_exemplars[0], \\\n",
    "        f\"Each author must have at least {nums_exemplars[0]} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy()\n",
    "    prompt_tmp = get_prompt_template_for_writing_setting1()        \n",
    "    \n",
    "    print(f\"Generating prompts...\")\n",
    "    out = []\n",
    "    exemplars_map = {}\n",
    "    for num_exemplar in nums_exemplars:\n",
    "        for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "            \n",
    "            author = row[author_col]\n",
    "            summary = row[summary_col]\n",
    "            \n",
    "            num_words = round_up_to_nearest_10(count_words(row[text_col]))\n",
    "\n",
    "            if ix in exemplars_map:\n",
    "                samples = exemplars_map[ix][:num_exemplar]\n",
    "            else:\n",
    "                samples = training_df[training_df[author_col]==author][text_col].sample(num_exemplar)\n",
    "                exemplars_map[ix] = samples\n",
    "            \n",
    "            writing_samples = list_writing_samples(samples)\n",
    "            prompt = prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                        genre=genre, num_words=num_words,\n",
    "                                        summary=summary)\n",
    "            evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "            evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "        out.append(evaluation_df.copy())\n",
    "        out[-1][\"num_exemplars\"] = num_exemplar\n",
    "    \n",
    "    out_df = pd.concat(out, axis=0).reset_index(drop=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a3ffce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        evaluation_df = create_writing_prompts_setting6(\n",
    "            training_df_fp=f\"dataset_followup/{dataset}_train.csv\",\n",
    "            evaluation_df_fp=f\"dataset_followup/{dataset}_test.csv\",\n",
    "            genre=\"blog\",\n",
    "            author_col=\"author\", \n",
    "            text_col=\"text\", \n",
    "            summary_col=\"summary\", \n",
    "            nums_exemplars=[2, 4, 6, 8, 10]\n",
    "        )\n",
    "        nums_exemplars = sorted(evaluation_df[\"num_exemplars\"].unique())\n",
    "\n",
    "        for n1, n2 in zip(nums_exemplars, nums_exemplars[1:]):\n",
    "            sub_n1 = evaluation_df[evaluation_df[\"num_exemplars\"]==n1].reset_index(drop=True)\n",
    "            sub_n2 = evaluation_df[evaluation_df[\"num_exemplars\"]==n2].reset_index(drop=True)\n",
    "            assert len(sub_n1) == len(sub_n2), \\\n",
    "                f\"Number of samples for num_exemplars {n1} and {n2} do not match. \" \\\n",
    "                f\"num_exemplars {n1}: {len(sub_n1)}, num_exemplars {n2}: {len(sub_n2)}\"\n",
    "            \n",
    "            for ixes1, ixes2 in zip(sub_n1[\"training sample indices\"], sub_n2[\"training sample indices\"]):\n",
    "                \n",
    "                assert ixes1 in ixes2, \\\n",
    "                    f\"Sample indices for num_exemplars {n1} not in num_exemplars {n2}. \" \\\n",
    "                    f\"Sample indices: {ixes1} not in {ixes2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e50b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 575.82it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 719.97it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 724.43it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 741.40it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 741.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 817.73it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1198.14it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1204.19it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1221.04it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1217.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 706.40it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1182.29it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1192.22it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1200.58it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1202.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 876.27it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1163.89it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1165.93it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1178.35it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1189.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cfd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1dcdadf",
   "metadata": {},
   "source": [
    "### Test on Jack's Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed4399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dc217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f50aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
