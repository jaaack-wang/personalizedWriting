{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8748277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19112a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86c0510",
   "metadata": {},
   "source": [
    "### Setting 2\n",
    "\n",
    "\n",
    "For each sample in the evaluation set, provide LLMs with 5 samples from the training data that belong to the particular cluster of the evaluation sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096448d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scripts.utils import count_words, round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting1\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting2(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    num_exemplars=5):\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= num_exemplars, \\\n",
    "        f\"Each author must have at least {num_exemplars} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "    \n",
    "    assert \"cluster\" in training_df.columns, \\\n",
    "        f\"Cluster column 'cluster' not found in training DataFrame.\"\n",
    "    \n",
    "    assert \"cluster\" in evaluation_df.columns, \\\n",
    "        f\"Cluster column 'cluster' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy()\n",
    "    prompt_tmp = get_prompt_template_for_writing_setting1()        \n",
    "    \n",
    "    print(f\"Generating prompts...\")\n",
    "    for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "        \n",
    "        author = row[author_col]\n",
    "        summary = row[summary_col]\n",
    "        cluster = row[\"cluster\"]\n",
    "        \n",
    "        num_words = round_up_to_nearest_10(count_words(row[text_col]))\n",
    "        samples = training_df[(training_df[author_col]==author) & \n",
    "                              (training_df[\"cluster\"]==cluster)][text_col].sample(num_exemplars)\n",
    "        \n",
    "        writing_samples = list_writing_samples(samples)\n",
    "        prompt = prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                       genre=genre, num_words=num_words,\n",
    "                                       summary=summary)\n",
    "        evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "        evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "    # evaluation_df.to_csv(evaluation_df_fp, index=False)\n",
    "    \n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39bc8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        evaluation_df = create_writing_prompts_setting2(\n",
    "            training_df_fp=f\"dataset_followup/{dataset}_train.csv\",\n",
    "            evaluation_df_fp=f\"dataset_followup/{dataset}_test.csv\",\n",
    "            genre=\"blog\",\n",
    "            author_col=\"author\", \n",
    "            text_col=\"text\", \n",
    "            summary_col=\"summary\", \n",
    "            num_exemplars=5\n",
    "        )\n",
    "        train = pd.read_csv(f\"dataset_followup/{dataset}_train.csv\")\n",
    "\n",
    "        for ix, row in evaluation_df.iterrows():\n",
    "            ixes = [int(i) for i in row[\"training sample indices\"].split(\",\")]\n",
    "            cluster = row[\"cluster\"]\n",
    "\n",
    "            cluster_train = train[train[\"cluster\"]==cluster][\"cluster\"].to_list()\n",
    "            assert len(set(cluster_train)) == 1, \\\n",
    "                f\"Cluster mismatch for author {row['author']} in dataset {dataset}. \" \\\n",
    "                f\"Expected cluster: {cluster}, Found clusters: {set(cluster_train)}\"\n",
    "            assert cluster_train[0] == cluster, \\\n",
    "                f\"Cluster mismatch for author {row['author']} in dataset {dataset}. \" \\\n",
    "                f\"Expected cluster: {cluster}, Found clusters: {set(cluster_train)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a5ea433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 568.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 785.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 674.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 828.41it/s]\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7182765",
   "metadata": {},
   "source": [
    "### Setting 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scripts.utils import count_words, round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting1\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting3(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    num_exemplars=5):\n",
    "    '''Create writing prompts for the evaluation set based on the training set.\n",
    "    For each sample in the evaluation set, find the num_exemplars most similar samples\n",
    "    in the training set based on word count. The prompt will include the writing samples\n",
    "    and the summary of the evaluation sample.\n",
    "    '''\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "    training_df[\"num_words\"] = training_df[text_col].apply(count_words)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= num_exemplars, \\\n",
    "        f\"Each author must have at least {num_exemplars} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy()\n",
    "    prompt_tmp = get_prompt_template_for_writing_setting1()        \n",
    "    \n",
    "    print(f\"Generating prompts...\")\n",
    "    for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "        \n",
    "        author = row[author_col]\n",
    "        summary = row[summary_col]\n",
    "        \n",
    "        num_words = count_words(row[text_col])\n",
    "        samples = training_df.copy()[training_df[author_col]==author]\n",
    "        samples[\"wc_diff\"] = abs(samples[\"num_words\"] - num_words)\n",
    "        samples = samples.sort_values(\"wc_diff\", ).head(num_exemplars)\n",
    "        \n",
    "        writing_samples = list_writing_samples(samples)\n",
    "        prompt = prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                       genre=genre, num_words=round_up_to_nearest_10(num_words),\n",
    "                                       summary=summary)\n",
    "        evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "        evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "    \n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332469a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e628fcbd",
   "metadata": {},
   "source": [
    "### Setting 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944da502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1974f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "910db0eb",
   "metadata": {},
   "source": [
    "### Setting 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65057324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scripts.utils import count_words, round_up_to_nearest_10, list_writing_samples\n",
    "from scripts.prompt_templates import get_prompt_template_for_writing_setting1\n",
    "\n",
    "\n",
    "def create_writing_prompts_setting6(training_df_fp, \n",
    "                                    evaluation_df_fp, \n",
    "                                    genre,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    summary_col=\"summary\", \n",
    "                                    nums_exemplars=[2, 4, 6, 8, 10]):\n",
    "    '''Similar to setting 1, but for each sample in the evaluation set,\n",
    "    find multiple num_exemplars random samples in the training set to be used as writing examples.\n",
    "    A larger nums_exemplar will subsume all smaller nums_exemplar. \n",
    "    '''\n",
    "    \n",
    "    training_df = pd.read_csv(training_df_fp)\n",
    "    evaluation_df = pd.read_csv(evaluation_df_fp)\n",
    "    nums_exemplars = sorted(nums_exemplars, reverse=True)\n",
    "\n",
    "    assert training_df[author_col].value_counts().min() >= nums_exemplars[0], \\\n",
    "        f\"Each author must have at least {nums_exemplars[0]} samples in the training set.\"\n",
    "    \n",
    "    assert summary_col in evaluation_df.columns, \\\n",
    "        f\"Summary column '{summary_col}' not found in evaluation DataFrame.\"\n",
    "\n",
    "    evaluation_df = evaluation_df.copy()\n",
    "    prompt_tmp = get_prompt_template_for_writing_setting1()        \n",
    "    \n",
    "    print(f\"Generating prompts...\")\n",
    "    out = []\n",
    "    exemplars_map = {}\n",
    "    for num_exemplar in nums_exemplars:\n",
    "        for ix, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df)):\n",
    "            \n",
    "            author = row[author_col]\n",
    "            summary = row[summary_col]\n",
    "            \n",
    "            num_words = round_up_to_nearest_10(count_words(row[text_col]))\n",
    "\n",
    "            if ix in exemplars_map:\n",
    "                samples = exemplars_map[ix][:num_exemplar]\n",
    "            else:\n",
    "                samples = training_df[training_df[author_col]==author][text_col].sample(num_exemplar)\n",
    "                exemplars_map[ix] = samples\n",
    "            \n",
    "            writing_samples = list_writing_samples(samples)\n",
    "            prompt = prompt_tmp.substitute(writing_samples=writing_samples, \n",
    "                                        genre=genre, num_words=num_words,\n",
    "                                        summary=summary)\n",
    "            evaluation_df.at[ix, \"training sample indices\"] = \",\".join([str(ix) for ix in samples.index])\n",
    "            evaluation_df.at[ix, \"prompt\"] = prompt\n",
    "\n",
    "        out.append(evaluation_df.copy())\n",
    "        out[-1][\"num_exemplars\"] = num_exemplar\n",
    "    \n",
    "    out_df = pd.concat(out, axis=0).reset_index(drop=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a3ffce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        evaluation_df = create_writing_prompts_setting6(\n",
    "            training_df_fp=f\"dataset_followup/{dataset}_train.csv\",\n",
    "            evaluation_df_fp=f\"dataset_followup/{dataset}_test.csv\",\n",
    "            genre=\"blog\",\n",
    "            author_col=\"author\", \n",
    "            text_col=\"text\", \n",
    "            summary_col=\"summary\", \n",
    "            nums_exemplars=[2, 4, 6, 8, 10]\n",
    "        )\n",
    "        nums_exemplars = sorted(evaluation_df[\"num_exemplars\"].unique())\n",
    "\n",
    "        for n1, n2 in zip(nums_exemplars, nums_exemplars[1:]):\n",
    "            sub_n1 = evaluation_df[evaluation_df[\"num_exemplars\"]==n1].reset_index(drop=True)\n",
    "            sub_n2 = evaluation_df[evaluation_df[\"num_exemplars\"]==n2].reset_index(drop=True)\n",
    "            assert len(sub_n1) == len(sub_n2), \\\n",
    "                f\"Number of samples for num_exemplars {n1} and {n2} do not match. \" \\\n",
    "                f\"num_exemplars {n1}: {len(sub_n1)}, num_exemplars {n2}: {len(sub_n2)}\"\n",
    "            \n",
    "            for ixes1, ixes2 in zip(sub_n1[\"training sample indices\"], sub_n2[\"training sample indices\"]):\n",
    "                \n",
    "                assert ixes1 in ixes2, \\\n",
    "                    f\"Sample indices for num_exemplars {n1} not in num_exemplars {n2}. \" \\\n",
    "                    f\"Sample indices: {ixes1} not in {ixes2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e50b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 575.82it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 719.97it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 724.43it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 741.40it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 741.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 817.73it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1198.14it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1204.19it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1221.04it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1217.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 706.40it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1182.29it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1192.22it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1200.58it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1202.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 876.27it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1163.89it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1165.93it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1178.35it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1189.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f50aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
