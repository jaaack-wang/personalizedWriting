{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dede3c",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Construct a dataset for each corpus for AV classifier training and testing\n",
    "2. Traing + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb07e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../scripts/create_AV_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/create_AV_datasets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "from itertools import combinations, product\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def construct_AV_dataset(df, \n",
    "                         author_col=\"author\", \n",
    "                         text_col=\"text\", \n",
    "                         num_of_samples=100000, \n",
    "                         pos_ratio=0.4):\n",
    "    cols = [\"author1\", \"author2\", \"text1\", \"text2\", \"label\"]\n",
    "    pos_samples = []\n",
    "    neg_samples = []\n",
    "\n",
    "    # Group texts by author\n",
    "    author_groups = df.groupby(author_col)\n",
    "\n",
    "    # Generate all possible positive pairs (same author)\n",
    "    for author, group in author_groups:\n",
    "        indices = group.index.tolist()\n",
    "        if len(indices) >= 2:\n",
    "            pos_samples.extend(combinations(indices, 2))\n",
    "\n",
    "    # Generate all possible negative pairs (different authors)\n",
    "    authors = df[author_col].unique()\n",
    "    author_indices = {author: df[df[author_col] == author].index.tolist() for author in authors}\n",
    "    author_list = list(author_indices.keys())\n",
    "\n",
    "    for i in range(len(author_list)):\n",
    "        for j in range(i+1, len(author_list)):\n",
    "            idx1 = author_indices[author_list[i]]\n",
    "            idx2 = author_indices[author_list[j]]\n",
    "            neg_samples.extend(product(idx1, idx2))\n",
    "\n",
    "    # Shuffle and sample\n",
    "    pos_needed = int(num_of_samples * pos_ratio)\n",
    "    neg_needed = num_of_samples - pos_needed\n",
    "\n",
    "    pos_pairs = sample(pos_samples, min(pos_needed, len(pos_samples)))\n",
    "    neg_pairs = sample(neg_samples, min(neg_needed, len(neg_samples)))\n",
    "\n",
    "    # Construct output\n",
    "    out = []\n",
    "    for ix1, ix2 in pos_pairs:\n",
    "        out.append([df.at[ix1, author_col], df.at[ix2, author_col], df.at[ix1, text_col], df.at[ix2, text_col], 1])\n",
    "    for ix1, ix2 in neg_pairs:\n",
    "        out.append([df.at[ix1, author_col], df.at[ix2, author_col], df.at[ix1, text_col], df.at[ix2, text_col], 0])\n",
    "\n",
    "    out_df = pd.DataFrame(out, columns=cols)\n",
    "    out_df = out_df.sample(frac=1).reset_index(drop=True)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def create_AV_train_valid_test_sets(fp_train, \n",
    "                                    save_dir,\n",
    "                                    fp_test=None,\n",
    "                                    author_col=\"author\", \n",
    "                                    text_col=\"text\", \n",
    "                                    num_of_samples_train=100000, \n",
    "                                    num_of_samples_test=20000,\n",
    "                                    valid_set_ratio=0.2,\n",
    "                                    pos_ratio=0.4):\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        print(f\"==> {save_dir} already exists. Please remove it to create a new dataset.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"===== Creating AV dataset for {save_dir} =====\")\n",
    "\n",
    "    df_train = pd.read_csv(fp_train)\n",
    "\n",
    "    if fp_test is None:\n",
    "        df_test = pd.read_csv(fp_train.replace(\"train\", \"test\"))\n",
    "    else:\n",
    "        df_test = pd.read_csv(fp_test)\n",
    "    \n",
    "    train_df = construct_AV_dataset(df_train, author_col, text_col, \n",
    "                                    num_of_samples_train, pos_ratio)\n",
    "    \n",
    "    train_df, valid_df = train_test_split(train_df, test_size=valid_set_ratio, random_state=42)\n",
    "    test_df = construct_AV_dataset(df_test, author_col, text_col, \n",
    "                                    num_of_samples_test, pos_ratio)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(save_dir, \"train.csv\"), index=False)\n",
    "    valid_df.to_csv(os.path.join(save_dir, \"valid.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(save_dir, \"test.csv\"), index=False)\n",
    "    print(f\"==> Train set size: {len(train_df)}\")\n",
    "    print(\"==> Train set label distribution:\\n\", train_df.label.value_counts(), \"\\n\\n\")\n",
    "\n",
    "    print(f\"==> Validation set size: {len(valid_df)}\")\n",
    "    print(\"==> Validation set label distribution:\\n\", valid_df.label.value_counts(), \"\\n\\n\")\n",
    "\n",
    "    print(f\"==> Test set size: {len(test_df)}\")\n",
    "    print(\"+=> Test set label distribution:\\n\", test_df.label.value_counts())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_AV_train_valid_test_sets(\"../dataset_prepare/blog_train.csv\", \"../dataset_prepare/blog_AV_datasets\")\n",
    "    create_AV_train_valid_test_sets(\"../dataset_prepare/CCAT50_train.csv\", \"../dataset_prepare/CCAT50_AV_datasets\")\n",
    "    create_AV_train_valid_test_sets(\"../dataset_prepare/enron_train.csv\", \"../dataset_prepare/enron_AV_datasets\")\n",
    "    create_AV_train_valid_test_sets(\"../dataset_prepare/reddit_train.csv\", \"../dataset_prepare/reddit_AV_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da65136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a32492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40993f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231d23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
