{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dede3c",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Construct a dataset for each corpus for AV classifier training and testing\n",
    "2. Traing + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../scripts/create_AV_datasets.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../scripts/create_AV_datasets.py\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from random import sample\n",
    "# from itertools import combinations, product\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# def construct_AV_dataset(df, \n",
    "#                          author_col=\"author\", \n",
    "#                          text_col=\"text\", \n",
    "#                          num_of_samples=100000, \n",
    "#                          pos_ratio=0.4):\n",
    "#     cols = [\"author1\", \"author2\", \"text1\", \"text2\", \"label\"]\n",
    "#     pos_samples = []\n",
    "#     neg_samples = []\n",
    "\n",
    "#     # Group texts by author\n",
    "#     author_groups = df.groupby(author_col)\n",
    "\n",
    "#     # Generate all possible positive pairs (same author)\n",
    "#     for author, group in author_groups:\n",
    "#         indices = group.index.tolist()\n",
    "#         if len(indices) >= 2:\n",
    "#             pos_samples.extend(combinations(indices, 2))\n",
    "\n",
    "#     # Generate all possible negative pairs (different authors)\n",
    "#     authors = df[author_col].unique()\n",
    "#     author_indices = {author: df[df[author_col] == author].index.tolist() for author in authors}\n",
    "#     author_list = list(author_indices.keys())\n",
    "\n",
    "#     for i in range(len(author_list)):\n",
    "#         for j in range(i+1, len(author_list)):\n",
    "#             idx1 = author_indices[author_list[i]]\n",
    "#             idx2 = author_indices[author_list[j]]\n",
    "#             neg_samples.extend(product(idx1, idx2))\n",
    "\n",
    "#     # Shuffle and sample\n",
    "#     pos_needed = int(num_of_samples * pos_ratio)\n",
    "#     neg_needed = num_of_samples - pos_needed\n",
    "\n",
    "#     pos_pairs = sample(pos_samples, min(pos_needed, len(pos_samples)))\n",
    "#     neg_pairs = sample(neg_samples, min(neg_needed, len(neg_samples)))\n",
    "\n",
    "#     # Construct output\n",
    "#     out = []\n",
    "#     for ix1, ix2 in pos_pairs:\n",
    "#         out.append([df.at[ix1, author_col], df.at[ix2, author_col], df.at[ix1, text_col], df.at[ix2, text_col], 1])\n",
    "#     for ix1, ix2 in neg_pairs:\n",
    "#         out.append([df.at[ix1, author_col], df.at[ix2, author_col], df.at[ix1, text_col], df.at[ix2, text_col], 0])\n",
    "\n",
    "#     out_df = pd.DataFrame(out, columns=cols)\n",
    "#     out_df = out_df.sample(frac=1).reset_index(drop=True)\n",
    "#     return out_df\n",
    "\n",
    "\n",
    "# def create_AV_train_valid_test_sets(fp_train, \n",
    "#                                     save_dir,\n",
    "#                                     fp_test=None,\n",
    "#                                     author_col=\"author\", \n",
    "#                                     text_col=\"text\", \n",
    "#                                     num_of_samples_train=100000, \n",
    "#                                     num_of_samples_test=20000,\n",
    "#                                     valid_set_ratio=0.2,\n",
    "#                                     pos_ratio=0.4):\n",
    "    \n",
    "#     if os.path.exists(save_dir):\n",
    "#         print(f\"==> {save_dir} already exists. Please remove it to create a new dataset.\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"===== Creating AV dataset for {save_dir} =====\")\n",
    "\n",
    "#     df_train = pd.read_csv(fp_train)\n",
    "\n",
    "#     if fp_test is None:\n",
    "#         df_test = pd.read_csv(fp_train.replace(\"train\", \"test\"))\n",
    "#     else:\n",
    "#         df_test = pd.read_csv(fp_test)\n",
    "    \n",
    "#     train_df = construct_AV_dataset(df_train, author_col, text_col, \n",
    "#                                     num_of_samples_train, pos_ratio)\n",
    "    \n",
    "#     train_df, valid_df = train_test_split(train_df, test_size=valid_set_ratio, random_state=42)\n",
    "#     test_df = construct_AV_dataset(df_test, author_col, text_col, \n",
    "#                                     num_of_samples_test, pos_ratio)\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     train_df.to_csv(os.path.join(save_dir, \"train.csv\"), index=False)\n",
    "#     valid_df.to_csv(os.path.join(save_dir, \"valid.csv\"), index=False)\n",
    "#     test_df.to_csv(os.path.join(save_dir, \"test.csv\"), index=False)\n",
    "#     print(f\"==> Train set size: {len(train_df)}\")\n",
    "#     print(\"==> Train set label distribution:\\n\", train_df.label.value_counts(), \"\\n\\n\")\n",
    "\n",
    "#     print(f\"==> Validation set size: {len(valid_df)}\")\n",
    "#     print(\"==> Validation set label distribution:\\n\", valid_df.label.value_counts(), \"\\n\\n\")\n",
    "\n",
    "#     print(f\"==> Test set size: {len(test_df)}\")\n",
    "#     print(\"+=> Test set label distribution:\\n\", test_df.label.value_counts())\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     create_AV_train_valid_test_sets(\"../dataset_prepare/blog_train.csv\", \"../dataset_prepare/blog_AV_datasets\")\n",
    "#     create_AV_train_valid_test_sets(\"../dataset_prepare/CCAT50_train.csv\", \"../dataset_prepare/CCAT50_AV_datasets\")\n",
    "#     create_AV_train_valid_test_sets(\"../dataset_prepare/enron_train.csv\", \"../dataset_prepare/enron_AV_datasets\")\n",
    "#     create_AV_train_valid_test_sets(\"../dataset_prepare/reddit_train.csv\", \"../dataset_prepare/reddit_AV_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102a504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac90ef8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6752122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../train_and_eval_an_AV_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../train_and_eval_an_AV_model.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Create AV datasets\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, required=True, help=\"Directory containing the dataset\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"allenai/longformer-base-4096\", help=\"Name of the model to be used\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=2048, help=\"Maximum length of the input sequences\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=8, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=16, help=\"Batch size for evaluation\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500, help=\"Number of warmup steps for learning rate scheduler\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for optimizer\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5, help=\"Learning rate for optimizer\")\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Logging steps\")\n",
    "    parser.add_argument(\"--evaluation_strategy\", type=str, default=\"epoch\", help=\"Evaluation strategy\")\n",
    "    parser.add_argument(\"--load_best_model_at_end\", type=str, default=\"True\", help=\"Load the best model at the end of training\")\n",
    "    parser.add_argument(\"--fp16\", type=str, default=\"True\", help=\"Use mixed precision training\")\n",
    "    parser.add_argument(\"--save_total_limit\", type=int, default=1, help=\"Limit the total amount of checkpoints\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./models\", help=\"Output directory for model checkpoints\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_AV_dataset(data_dire):\n",
    "    train_df = pd.read_csv(os.path.join(data_dire, \"train.csv\"))\n",
    "    valid_df = pd.read_csv(os.path.join(data_dire, \"valid.csv\"))\n",
    "    test_df = pd.read_csv(os.path.join(data_dire, \"test.csv\"))\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "def bool_str_to_bool(value):\n",
    "    return value.lower() in ('true', '1', 'yes', 'y', 't')\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_metric.add_batch(predictions=predictions, references=labels)\n",
    "    return f1_metric.compute()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    \n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the datasets\n",
    "    train_df, valid_df, test_df = load_AV_dataset(args.data_dir)\n",
    "    print(f\"Loaded datasets from {args.data_dir}\")\n",
    "\n",
    "    # Load the tokenizer and tokenize the datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    print(f\"Loaded tokenizer from {args.model_name}\")\n",
    "\n",
    "    train_df = train_df.copy()[:1000]\n",
    "    valid_df = valid_df.copy()[:1000]\n",
    "    test_df = test_df.copy()[:1000]\n",
    "\n",
    "    train_encodings = tokenizer(list(train_df['text1']), list(train_df['text2']), \n",
    "                                truncation=True, padding=\"max_length\", max_length=args.max_length)\n",
    "    valid_encodings = tokenizer(list(valid_df['text1']), list(valid_df['text2']), truncation=True, \n",
    "                                padding=\"max_length\", max_length=args.max_length)\n",
    "    test_encodings = tokenizer(list(test_df['text1']), list(test_df['text2']), truncation=True, \n",
    "                               padding=\"max_length\", max_length=args.max_length)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
    "    valid_dataset = CustomDataset(valid_encodings, valid_df['label'].tolist())\n",
    "    test_dataset = CustomDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "    # Load the Longformer model for sequence classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=2)\n",
    "\n",
    "    dataset = args.data_dir.split(\"/\")[-1]\n",
    "    model_output_dir = f\"./{args.model_name.split('/')[-1]}_models//\" + dataset\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,  # Output directory\n",
    "        fp16=bool_str_to_bool(args.fp16),  # Use mixed precision training\n",
    "        num_train_epochs=args.num_train_epochs,  # Total number of training epochs\n",
    "        per_device_train_batch_size=args.train_batch_size,  # Batch size per device during training\n",
    "        per_device_eval_batch_size=args.eval_batch_size,  # Batch size for evaluation\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,  # Number of updates steps to accumulate before performing a backward/update pass\n",
    "        warmup_steps=args.warmup_steps,  # Number of warmup steps for learning rate scheduler\n",
    "        weight_decay=args.weight_decay,  # Strength of weight decay\n",
    "        learning_rate=args.learning_rate,  # Initial learning rate\n",
    "        save_total_limit=args.save_total_limit,  # Limit the total amount of checkpoints\n",
    "        logging_steps=args.logging_steps,  # Log every X updates steps\n",
    "        eval_strategy=args.evaluation_strategy,  # Evaluation strategy to adopt during training\n",
    "        save_strategy=args.evaluation_strategy,  # Save strategy to adopt during training\n",
    "        load_best_model_at_end= bool_str_to_bool(args.load_best_model_at_end),  # Load the best model at the end of training\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the Trainer object\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = predictions.predictions.argmax(-1)\n",
    "\n",
    "    y_test = test_df.label.tolist()\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    logits = predictions.predictions  # This contains the raw logits output\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = softmax(torch.tensor(logits), dim=1).tolist()\n",
    "    test_df[\"prediction\"]=y_pred\n",
    "    test_df[\"probabilities\"] = [prob[1] for prob in probabilities]\n",
    "    test_df.to_csv(os.path.join(args.data_dir, \"test_results.csv\"), index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84588ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0182c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb2400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f04a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40993f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3231d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'enron_email'\n",
    "col_1 = 'longformer_prediction'\n",
    "col_2 = 'longformer_prob'\n",
    "model_output_dir = \"./AV_longformer_models//\"+dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad890355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 0    48020\n",
       " 1    31980\n",
       " Name: count, dtype: int64,\n",
       " label\n",
       " 0    11980\n",
       " 1     8020\n",
       " Name: count, dtype: int64,\n",
       " label\n",
       " 0    12000\n",
       " 1     8000\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_AV_dataset(data_dire):\n",
    "    train_df = pd.read_csv(os.path.join(data_dire, \"train.csv\"))\n",
    "    valid_df = pd.read_csv(os.path.join(data_dire, \"valid.csv\"))\n",
    "    test_df = pd.read_csv(os.path.join(data_dire, \"test.csv\"))\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "train_df, valid_df, test_df = load_AV_dataset(\"../dataset_prepare/enron_AV_datasets\")\n",
    "train_df.label.value_counts(), valid_df.label.value_counts(), test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3702a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'enron_email'\n",
    "col_1 = 'longformer_prediction'\n",
    "col_2 = 'longformer_prob'\n",
    "model_output_dir = \"./AV_longformer_models//\"+dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ac560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Load the Longformer tokenizer\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the datasets with max_length set to 4096\n",
    "train_df = train_df.copy()[:1000]\n",
    "valid_df = valid_df.copy()[:1000]\n",
    "test_df = test_df.copy()[:1000]\n",
    "\n",
    "train_encodings = tokenizer(list(train_df['text1']), list(train_df['text2']), truncation=True, padding=\"max_length\", max_length=2048)\n",
    "valid_encodings = tokenizer(list(valid_df['text1']), list(valid_df['text2']), truncation=True, padding=\"max_length\", max_length=2048)\n",
    "test_encodings = tokenizer(list(test_df['text1']), list(test_df['text2']), truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
    "valid_dataset = CustomDataset(valid_encodings, valid_df['label'].tolist())\n",
    "test_dataset = CustomDataset(test_encodings, test_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54c7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_metric.add_batch(predictions=predictions, references=labels)\n",
    "    return f1_metric.compute()\n",
    "\n",
    "# Load the Longformer model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,  # Output directory\n",
    "    fp16=True,\n",
    "    num_train_epochs=1,  # Total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate batch size of 16\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7265706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 03:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672696</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31, training_loss=0.6791960193264869, metrics={'train_runtime': 216.9593, 'train_samples_per_second': 4.609, 'train_steps_per_second': 0.143, 'total_flos': 1303193597509632.0, 'train_loss': 0.6791960193264869, 'epoch': 0.992})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2588ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6726962924003601,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_runtime': 88.3509,\n",
       " 'eval_samples_per_second': 11.319,\n",
       " 'eval_steps_per_second': 0.713,\n",
       " 'epoch': 0.992}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0851d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad623c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17 588]\n",
      " [  4 391]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.03      0.05       605\n",
      "           1       0.40      0.99      0.57       395\n",
      "\n",
      "    accuracy                           0.41      1000\n",
      "   macro avg       0.60      0.51      0.31      1000\n",
      "weighted avg       0.65      0.41      0.26      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "\n",
    "y_test = test_df.label.tolist()\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3edf2da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669e278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author1</th>\n",
       "      <th>author2</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "      <th>longformer_prediction</th>\n",
       "      <th>longformer_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Beck</td>\n",
       "      <td>Sally Beck</td>\n",
       "      <td>I will be out of the office for most of the da...</td>\n",
       "      <td>Mary Solmonson, one of my direct reports, has ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sara Shackleton</td>\n",
       "      <td>Wordsmith</td>\n",
       "      <td>Tanya:  There's no short answer with these peo...</td>\n",
       "      <td>punctilious (pungk-TIL-ee-uhs) adjective\\n\\n  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bruno Gaillard</td>\n",
       "      <td>Bruno Gaillard</td>\n",
       "      <td>SB 1712, Author, Polanco.  Universal communica...</td>\n",
       "      <td>AB 2198 Telecommunications: local telephone se...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sally Beck</td>\n",
       "      <td>Sally Beck</td>\n",
       "      <td>Contact numbers are listed below for Greg Pipe...</td>\n",
       "      <td>I will be on vacation the week of March while ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Grigsby</td>\n",
       "      <td>Marie Heard</td>\n",
       "      <td>Dear John,\\n\\nI had a meeting with Phillip aft...</td>\n",
       "      <td>Hi, Tanya!\\n\\nJay said that he has passed AK S...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Kimberly Watson</td>\n",
       "      <td>Michelle Cash</td>\n",
       "      <td>Earl and Mansoor,\\n\\nI have received a copy of...</td>\n",
       "      <td>Dee,\\n\\nI left you a voice mail yesterday, but...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Susan Bailey</td>\n",
       "      <td>Mark Greenberg</td>\n",
       "      <td>We have received an executed Master Agreement:...</td>\n",
       "      <td>Julia/Alan/Mark -\\n\\nMark asked me to work on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.522552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Susan M Scott</td>\n",
       "      <td>Mary Cook</td>\n",
       "      <td>Fifth grade assignment\\n\\nThe teacher gave her...</td>\n",
       "      <td>1.  I was not sure whose doc the referenced no...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Pete Davis</td>\n",
       "      <td>Pete Davis</td>\n",
       "      <td>\\n\\nStart Date: 1/21/02; HourAhead hour: 16;  ...</td>\n",
       "      <td>\\n\\nStart Date: 10/23/01; HourAhead hour: 21; ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Sara Shackleton</td>\n",
       "      <td>Sara Shackleton</td>\n",
       "      <td>A few questions:\\n\\n(1)  Where does the NYMEX ...</td>\n",
       "      <td>Sorry to have to respond via e-mail but I've g...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author1          author2  \\\n",
       "0         Sally Beck       Sally Beck   \n",
       "1    Sara Shackleton       Wordsmith    \n",
       "2     Bruno Gaillard   Bruno Gaillard   \n",
       "3         Sally Beck       Sally Beck   \n",
       "4       Mike Grigsby      Marie Heard   \n",
       "..               ...              ...   \n",
       "995  Kimberly Watson    Michelle Cash   \n",
       "996     Susan Bailey   Mark Greenberg   \n",
       "997    Susan M Scott        Mary Cook   \n",
       "998       Pete Davis       Pete Davis   \n",
       "999  Sara Shackleton  Sara Shackleton   \n",
       "\n",
       "                                                 text1  \\\n",
       "0    I will be out of the office for most of the da...   \n",
       "1    Tanya:  There's no short answer with these peo...   \n",
       "2    SB 1712, Author, Polanco.  Universal communica...   \n",
       "3    Contact numbers are listed below for Greg Pipe...   \n",
       "4    Dear John,\\n\\nI had a meeting with Phillip aft...   \n",
       "..                                                 ...   \n",
       "995  Earl and Mansoor,\\n\\nI have received a copy of...   \n",
       "996  We have received an executed Master Agreement:...   \n",
       "997  Fifth grade assignment\\n\\nThe teacher gave her...   \n",
       "998  \\n\\nStart Date: 1/21/02; HourAhead hour: 16;  ...   \n",
       "999  A few questions:\\n\\n(1)  Where does the NYMEX ...   \n",
       "\n",
       "                                                 text2  label  \\\n",
       "0    Mary Solmonson, one of my direct reports, has ...      1   \n",
       "1    punctilious (pungk-TIL-ee-uhs) adjective\\n\\n  ...      0   \n",
       "2    AB 2198 Telecommunications: local telephone se...      1   \n",
       "3    I will be on vacation the week of March while ...      1   \n",
       "4    Hi, Tanya!\\n\\nJay said that he has passed AK S...      0   \n",
       "..                                                 ...    ...   \n",
       "995  Dee,\\n\\nI left you a voice mail yesterday, but...      0   \n",
       "996  Julia/Alan/Mark -\\n\\nMark asked me to work on ...      0   \n",
       "997  1.  I was not sure whose doc the referenced no...      0   \n",
       "998  \\n\\nStart Date: 10/23/01; HourAhead hour: 21; ...      1   \n",
       "999  Sorry to have to respond via e-mail but I've g...      1   \n",
       "\n",
       "     longformer_prediction  longformer_prob  \n",
       "0                        1         0.517556  \n",
       "1                        1         0.527005  \n",
       "2                        1         0.529499  \n",
       "3                        1         0.547146  \n",
       "4                        1         0.527708  \n",
       "..                     ...              ...  \n",
       "995                      1         0.533542  \n",
       "996                      1         0.522552  \n",
       "997                      1         0.511511  \n",
       "998                      1         0.523344  \n",
       "999                      1         0.530974  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract logits from predictions\n",
    "from torch.nn.functional import softmax\n",
    "logits = predictions.predictions  # This contains the raw logits output\n",
    "# Convert logits to probabilities using softmax\n",
    "probabilities = softmax(torch.tensor(logits), dim=1).tolist()\n",
    "# Extract probabilities for class 1\n",
    "class_1_probabilities = [prob[1] for prob in probabilities]\n",
    "test_df[col_1]=y_pred\n",
    "test_df[col_2] = class_1_probabilities\n",
    "# new_test_file_name =  'predictions//'+corpora+'_av_prediction_'+dataset+'.csv'\n",
    "# test_df.to_csv(new_test_file_name,index=False,header=True, sep='\\t')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20f0430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.408)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_df.label == test_df[col_1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3d90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aba278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c992cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
