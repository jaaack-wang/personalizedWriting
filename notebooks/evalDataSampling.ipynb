{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fdda8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'file_name', 'summary', 'training sample indices',\n",
       "       'prompt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"LLM_writing/Setting1/CCAT50/prompts.csv\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b6a71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'subject', 'summary', 'prompt',\n",
       "       'training sample indices'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"LLM_writing/Setting1/enron/prompts.csv\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a5fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'author', 'text', 'subreddit', 'summary',\n",
       "       'training sample indices', 'prompt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"LLM_writing/Setting1/reddit/prompts.csv\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0449f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9abf4067",
   "metadata": {},
   "source": [
    "### Sampling Criteria\n",
    "\n",
    "- 1. Sample up to 50 authors per dataset\n",
    "- 2. Each author should have 10 samples per evaluation dataset\n",
    "- 3. Each author in the evaluation data should at least form 2 clusters\n",
    "- 4. These 2 clusters must also appear in the train sets, with each cluster have at least 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "507514d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    train = pd.read_csv(f\"dataset_prepare/{dataset}_train.csv\")\n",
    "    test = pd.read_csv(f\"dataset_prepare/{dataset}_test.csv\")\n",
    "    test = test[train.columns.to_list() + [\"summary\"]]\n",
    "    train[\"split\"] = \"train\"\n",
    "    test[\"split\"] = \"test\"\n",
    "    return pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "\n",
    "def berttopic_clustering(docs):\n",
    "    topic_model = BERTopic()\n",
    "    topics, _ = topic_model.fit_transform(docs)\n",
    "    return topics\n",
    "\n",
    "\n",
    "def topic_model_a_dataset(dataset):\n",
    "    df = load_dataset(dataset)\n",
    "\n",
    "    for author in tqdm(df.author.unique()):\n",
    "        sub = df[df.author == author]\n",
    "        docs = sub.text.tolist()\n",
    "        topics = berttopic_clustering(docs)\n",
    "\n",
    "        for j, ix in enumerate(sub.index):\n",
    "            df.loc[ix, \"topic\"] = topics[j]\n",
    "    \n",
    "    test = df[df.split == \"test\"]\n",
    "    test.drop(columns=[\"split\"], inplace=True)\n",
    "    train = df[df.split == \"train\"]\n",
    "    train.drop(columns=[\"split\"], inplace=True)\n",
    "\n",
    "    original_train = pd.read_csv(f\"dataset_prepare/{dataset}_train.csv\")\n",
    "    assert train.shape[0] == original_train.shape[0], \"Train set size mismatch after topic modeling.\"\n",
    "    assert train[[\"author\", \"text\"]].equals(original_train[[\"author\", \"text\"]]), \"Train set content mismatch after topic modeling.\"\n",
    "\n",
    "    test = test[test.topic != -1]\n",
    "    authors_to_keep = []\n",
    "    for author in test.author.unique():\n",
    "        sub = test[test.author == author]\n",
    "\n",
    "        # Check if the author has at least 10 samples in the test set with more than 1 topic\n",
    "        if len(sub) >= 10 and len(sub.topic.unique()) > 1:\n",
    "            test_topics = sub.topic.unique()\n",
    "            train_sub = train[train.author == author]\n",
    "\n",
    "            # Check if the author has at least 10 samples in the train set for each topic\n",
    "            to_add = True\n",
    "\n",
    "            for topic in test_topics:\n",
    "                train_sub_sub = train_sub[train_sub.topic == topic]\n",
    "                if len(train_sub_sub) < 5:\n",
    "                    to_add = False\n",
    "\n",
    "            if to_add:\n",
    "                authors_to_keep.append(author)\n",
    "\n",
    "    # test = test[test.author.isin(authors_to_keep)]\n",
    "    test_new = []\n",
    "    for author in authors_to_keep:\n",
    "        sub = test[test.author == author].sample(10)\n",
    "        test_new.append(sub)\n",
    "\n",
    "    test = pd.concat(test_new, ignore_index=True)\n",
    "    \n",
    "    save_dir = \"dataset_followup\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train.to_csv(f\"{save_dir}/{dataset}_train.csv\", index=False)\n",
    "    test.to_csv(f\"{save_dir}/{dataset}_test.csv\", index=False)\n",
    "    print(f\"Saved {dataset} dataset with topics to {save_dir} folder.\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f1f43043",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['summary'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model_a_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menron\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique authors in test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test\u001b[38;5;241m.\u001b[39mauthor\u001b[38;5;241m.\u001b[39munique()))\n",
      "Cell \u001b[0;32mIn[148], line 23\u001b[0m, in \u001b[0;36mtopic_model_a_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtopic_model_a_dataset\u001b[39m(dataset):\n\u001b[0;32m---> 23\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39mauthor\u001b[38;5;241m.\u001b[39munique()):\n\u001b[1;32m     26\u001b[0m         sub \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mauthor \u001b[38;5;241m==\u001b[39m author]\n",
      "Cell \u001b[0;32mIn[148], line 10\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      8\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_prepare/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_prepare/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['summary'] not in index\""
     ]
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"enron\")\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "949418ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:09<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved blog dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 78\n"
     ]
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"blog\")\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5b7f392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CCAT50 dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"CCAT50\")\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6b29b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:41<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reddit dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 95\n"
     ]
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"reddit\")\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5af268db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>AA-label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1142426</td>\n",
       "      <td>CaspianX2</td>\n",
       "      <td>I'm not sure how you can call the BLS stats \"j...</td>\n",
       "      <td>politics</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>566030</td>\n",
       "      <td>CaspianX2</td>\n",
       "      <td>Even if Fluke was funded by a lobbying group t...</td>\n",
       "      <td>politics</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1224678</td>\n",
       "      <td>CaspianX2</td>\n",
       "      <td>Okay, I'll try to condense it down as best I c...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>379826</td>\n",
       "      <td>CaspianX2</td>\n",
       "      <td>Actually, in a way, the rest of the world does...</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>607184</td>\n",
       "      <td>CaspianX2</td>\n",
       "      <td>Umm... Super Mario RPG is on Virtual Console, ...</td>\n",
       "      <td>gaming</td>\n",
       "      <td>39</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>1236798</td>\n",
       "      <td>ZeNuGerman</td>\n",
       "      <td>I do machine learning, and thus routinely have...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>68</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>582704</td>\n",
       "      <td>ZeNuGerman</td>\n",
       "      <td>The issue here, though, is historical context....</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>38225</td>\n",
       "      <td>ZeNuGerman</td>\n",
       "      <td>Add enough transistors and you could simply le...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>68</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>557901</td>\n",
       "      <td>ZeNuGerman</td>\n",
       "      <td>Thank you for your thoughtful reply. I do agre...</td>\n",
       "      <td>atheism</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>1322146</td>\n",
       "      <td>ZeNuGerman</td>\n",
       "      <td>Close, but no cigar. \\nEverybody wants to have...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      author                                               text  \\\n",
       "0    1142426   CaspianX2  I'm not sure how you can call the BLS stats \"j...   \n",
       "1     566030   CaspianX2  Even if Fluke was funded by a lobbying group t...   \n",
       "2    1224678   CaspianX2  Okay, I'll try to condense it down as best I c...   \n",
       "3     379826   CaspianX2  Actually, in a way, the rest of the world does...   \n",
       "4     607184   CaspianX2  Umm... Super Mario RPG is on Virtual Console, ...   \n",
       "..       ...         ...                                                ...   \n",
       "945  1236798  ZeNuGerman  I do machine learning, and thus routinely have...   \n",
       "946   582704  ZeNuGerman  The issue here, though, is historical context....   \n",
       "947    38225  ZeNuGerman  Add enough transistors and you could simply le...   \n",
       "948   557901  ZeNuGerman  Thank you for your thoughtful reply. I do agre...   \n",
       "949  1322146  ZeNuGerman  Close, but no cigar. \\nEverybody wants to have...   \n",
       "\n",
       "             subreddit  AA-label  topic  \n",
       "0             politics        39    0.0  \n",
       "1             politics        39    0.0  \n",
       "2               AskMen        39    1.0  \n",
       "3    explainlikeimfive        39    0.0  \n",
       "4               gaming        39    2.0  \n",
       "..                 ...       ...    ...  \n",
       "945          AskReddit        68    3.0  \n",
       "946          AskReddit        68    1.0  \n",
       "947          AskReddit        68    3.0  \n",
       "948            atheism        68    1.0  \n",
       "949          AskReddit        68    0.0  \n",
       "\n",
       "[950 rows x 6 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "025eaa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'file_name', 'AA-label',\n",
       "       'longformer-base-4096-AA-prediction',\n",
       "       'longformer-base-4096-AA-probabilities',\n",
       "       'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-predictions',\n",
       "       'ModernBERT-base-AA-top_k-probabilities',\n",
       "       'ModernBERT-base-AA-top_k-predictions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_prepare/CCAT50_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1554c4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'author', 'text', 'subreddit', 'AA-label',\n",
       "       'longformer-base-4096-AA-prediction',\n",
       "       'longformer-base-4096-AA-probabilities',\n",
       "       'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-predictions',\n",
       "       'ModernBERT-base-AA-top_k-probabilities',\n",
       "       'ModernBERT-base-AA-top_k-predictions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_prepare/reddit_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a4685adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'subject', 'AA-label',\n",
       "       'bert-base-uncased-AA-prediction', 'bert-base-uncased-AA-probabilities',\n",
       "       'longformer-base-4096-AA-prediction',\n",
       "       'longformer-base-4096-AA-probabilities',\n",
       "       'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-predictions',\n",
       "       'ModernBERT-base-AA-top_k-probabilities',\n",
       "       'ModernBERT-base-AA-top_k-predictions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_prepare/enron_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7ba59907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'topic', 'gender', 'age', 'sign', 'date', 'summary',\n",
       "       'training sample indices', 'prompt', 'label',\n",
       "       'bert-base-uncased-prediction', 'bert-base-uncased-probabilities',\n",
       "       'AA-label', 'longformer-base-4096-AA-prediction',\n",
       "       'longformer-base-4096-AA-probabilities',\n",
       "       'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-predictions',\n",
       "       'ModernBERT-base-AA-top_k-probabilities',\n",
       "       'ModernBERT-base-AA-top_k-predictions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_prepare/blog_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a44596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
