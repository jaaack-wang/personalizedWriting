{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0449f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9abf4067",
   "metadata": {},
   "source": [
    "### Sampling Criteria\n",
    "\n",
    "- 2. Each author should have 10 samples per evaluation dataset\n",
    "- 3. Each author in the evaluation data should at least form 2 non-outlier clusters\n",
    "- 4. These 2 clusters must also appear in the train sets, with each cluster have at least 5 samples for each non-outlier cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507514d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-08 20:24:39.540327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746750279.551866 2864383 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746750279.555305 2864383 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746750279.565864 2864383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746750279.565874 2864383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746750279.565876 2864383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746750279.565877 2864383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 20:24:39.568932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    train = pd.read_csv(f\"dataset_prepare/{dataset}_train.csv\")\n",
    "    test = pd.read_csv(f\"dataset_prepare/{dataset}_test.csv\")\n",
    "    test = test[train.columns.to_list() + [\"summary\"]]\n",
    "    train[\"split\"] = \"train\"\n",
    "    test[\"split\"] = \"test\"\n",
    "    return pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "\n",
    "def berttopic_clustering(docs):\n",
    "    topic_model = BERTopic()\n",
    "    topics, _ = topic_model.fit_transform(docs)\n",
    "    return topics\n",
    "\n",
    "\n",
    "def topic_model_a_dataset(dataset, max_num_authors=50):\n",
    "    \"\"\"\n",
    "    This function loads a dataset, applies BERTopic clustering to the text data,\n",
    "    and saves the resulting train and test sets with topic labels. \n",
    "\n",
    "    Parameters:\n",
    "    - dataset (str): The name of the dataset to be processed.\n",
    "    - max_num_authors (int): The maximum number of most frequently occuring authors to keep in the test set. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = load_dataset(dataset)\n",
    "\n",
    "    for author in tqdm(df.author.unique()):\n",
    "        sub = df[df.author == author]\n",
    "        docs = sub.text.tolist()\n",
    "        topics = berttopic_clustering(docs)\n",
    "\n",
    "        for j, ix in enumerate(sub.index):\n",
    "            df.loc[ix, \"cluster\"] = topics[j]\n",
    "    \n",
    "    test = df[df.split == \"test\"]\n",
    "    test.drop(columns=[\"split\"], inplace=True)\n",
    "    train = df[df.split == \"train\"]\n",
    "    train.drop(columns=[\"summary\", \"split\"], inplace=True)\n",
    "\n",
    "    original_train = pd.read_csv(f\"dataset_prepare/{dataset}_train.csv\")\n",
    "    assert train.shape[0] == original_train.shape[0], \"Train set size mismatch after topic modeling.\"\n",
    "    assert train[[\"author\", \"text\"]].equals(original_train[[\"author\", \"text\"]]), \"Train set content mismatch after topic modeling.\"\n",
    "\n",
    "    test = test[test.cluster != -1]\n",
    "    authors_to_keep = []\n",
    "    for author in test.author.value_counts().index:\n",
    "        sub = test[test.author == author]\n",
    "\n",
    "        # Check if the author has at least 10 samples in the test set with more than 1 cluster\n",
    "        if len(sub) >= 10 and len(sub.cluster.unique()) > 1:\n",
    "            test_clusters = sub.cluster.unique()\n",
    "            train_sub = train[train.author == author]\n",
    "\n",
    "            # Check if the author has at least 10 samples in the train set for each cluster\n",
    "            to_add = True\n",
    "\n",
    "            for cluster in test_clusters:\n",
    "                train_sub_sub = train_sub[train_sub.cluster == cluster]\n",
    "                if len(train_sub_sub) < 5:\n",
    "                    to_add = False\n",
    "\n",
    "            if to_add:\n",
    "                authors_to_keep.append(author)\n",
    "\n",
    "    # test = test[test.author.isin(authors_to_keep)]\n",
    "    test_new = []\n",
    "    for author in authors_to_keep[:max_num_authors]:\n",
    "        sub = test[test.author == author].sample(10)\n",
    "        test_new.append(sub)\n",
    "\n",
    "    test = pd.concat(test_new, ignore_index=True)\n",
    "    \n",
    "    save_dir = \"dataset_followup\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train.to_csv(f\"{save_dir}/{dataset}_train.csv\", index=False)\n",
    "    test.to_csv(f\"{save_dir}/{dataset}_test.csv\", index=False)\n",
    "    print(f\"Saved {dataset} dataset with topics to {save_dir} folder.\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f43043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:28<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enron dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'subject', 'AA-label', 'summary', 'cluster'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"enron\", max_num_authors=30)\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949418ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:46<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved blog dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'topic', 'gender', 'age', 'sign', 'date', 'AA-label',\n",
       "       'summary', 'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"blog\", max_num_authors=50)\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7f392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CCAT50 dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'file_name', 'AA-label', 'summary', 'cluster'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"CCAT50\", max_num_authors=30)\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b29b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reddit dataset with topics to dataset_followup folder.\n",
      "Number of unique authors in test set: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['index', 'author', 'text', 'subreddit', 'AA-label', 'summary',\n",
       "       'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = topic_model_a_dataset(\"reddit\", max_num_authors=50)\n",
    "print(\"Number of unique authors in test set:\", len(test.author.unique()))\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9699dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2823f8aa",
   "metadata": {},
   "source": [
    "### Double Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025eaa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'file_name', 'AA-label', 'summary', 'cluster'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_followup/CCAT50_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1554c4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'author', 'text', 'subreddit', 'AA-label', 'summary',\n",
       "       'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_followup/reddit_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4685adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'subject', 'AA-label', 'summary', 'cluster'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_followup/enron_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba59907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'text', 'topic', 'gender', 'age', 'sign', 'date', 'AA-label',\n",
       "       'summary', 'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = pd.read_csv(\"dataset_followup/blog_test.csv\")\n",
    "blog_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee7467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
