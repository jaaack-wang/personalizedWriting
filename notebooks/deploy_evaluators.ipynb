{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0463ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9b001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e24af92",
   "metadata": {},
   "source": [
    "#### Helper Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a514ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.utils import get_filepathes_from_dir\n",
    "\n",
    "\n",
    "def find_all_LLM_generated_writing_fps(LLM_writing_dir):\n",
    "    \"\"\"\n",
    "    Find all fps in the given directory that contain LLM-generated writing.\n",
    "    \"\"\"\n",
    "    # Get all file paths in the directory\n",
    "    file_paths = get_filepathes_from_dir(LLM_writing_dir, \n",
    "                                         include_sub_dir=True, \n",
    "                                         file_format=\".csv\")\n",
    "    \n",
    "    assert any(fp.endswith(\"prompts.csv\") for fp in file_paths), \\\n",
    "        \"No prompts.csv files found in the directory. Not the right directory?\" \n",
    "    # Filter out files that contain LLM-generated writing\n",
    "    llm_fps = [fp for fp in file_paths if not fp.endswith(\"prompts.csv\")]\n",
    "\n",
    "    return llm_fps\n",
    "\n",
    "\n",
    "def load_the_corresponding_prompts(llm_fp):\n",
    "    \"\"\"\n",
    "    Load the corresponding prompts for the given LLM-generated writing file path.\n",
    "    \"\"\"\n",
    "    # Get the corresponding prompts file path\n",
    "    parent_dire = os.path.dirname(llm_fp)\n",
    "    prompts_fp = os.path.join(parent_dire, \"prompts.csv\")\n",
    "    \n",
    "    # Check if the prompts file exists\n",
    "    if not os.path.exists(prompts_fp):\n",
    "        raise FileNotFoundError(f\"Prompts file not found: {prompts_fp}\")\n",
    "    \n",
    "    return pd.read_csv(prompts_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0bfa745",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_fps = find_all_LLM_generated_writing_fps(\"LLM_writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94e993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "502e5c93",
   "metadata": {},
   "source": [
    "### AA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84e4f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['author', 'text', 'topic', 'gender', 'age', 'sign', 'date', 'summary',\n",
       "        'training sample indices', 'prompt', 'label',\n",
       "        'bert-base-uncased-prediction', 'bert-base-uncased-probabilities',\n",
       "        'AA-label', 'longformer-base-4096-AA-prediction',\n",
       "        'longformer-base-4096-AA-probabilities',\n",
       "        'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities'],\n",
       "       dtype='object'),\n",
       " Index(['author', 'text', 'file_name', 'AA-label',\n",
       "        'longformer-base-4096-AA-prediction',\n",
       "        'longformer-base-4096-AA-probabilities',\n",
       "        'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities'],\n",
       "       dtype='object'),\n",
       " Index(['author', 'text', 'subject', 'AA-label',\n",
       "        'bert-base-uncased-AA-prediction', 'bert-base-uncased-AA-probabilities',\n",
       "        'longformer-base-4096-AA-prediction',\n",
       "        'longformer-base-4096-AA-probabilities',\n",
       "        'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities'],\n",
       "       dtype='object'),\n",
       " Index(['index', 'author', 'text', 'subreddit', 'AA-label',\n",
       "        'longformer-base-4096-AA-prediction',\n",
       "        'longformer-base-4096-AA-probabilities',\n",
       "        'ModernBERT-base-AA-prediction', 'ModernBERT-base-AA-probabilities'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_test = pd.read_csv(\"dataset_prepare/blog_test.csv\")\n",
    "df2_test = pd.read_csv(\"dataset_prepare/CCAT50_test.csv\")\n",
    "df3_test = pd.read_csv(\"dataset_prepare/enron_test.csv\")\n",
    "df4_test = pd.read_csv(\"dataset_prepare/reddit_test.csv\")\n",
    "\n",
    "df1_test.columns, df2_test.columns, df3_test.columns, df4_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# def get_report_from_df(df, dataset):\n",
    "#     out = []\n",
    "#     cols = [\"dataset\", \"# classes\", \"min_class_num\", \"avg_class_num\", \"model\", \"accuracy\"]\n",
    "#     models_pred = [col for col in df.columns if col.endswith(\"-AA-prediction\")]\n",
    "#     labels = df[\"AA-label\"].tolist()\n",
    "#     number_of_classes = len(set(labels))\n",
    "#     min_class_num = df[\"AA-label\"].value_counts().min()\n",
    "#     avg_class_num = df[\"AA-label\"].value_counts().mean()\n",
    "#     for model_pred in models_pred:\n",
    "#         preds = df[model_pred].tolist()\n",
    "#         accu = accuracy_score(labels, preds)\n",
    "#         model = model_pred.split(\"-\")[0]\n",
    "#         out.append([dataset, number_of_classes, min_class_num, avg_class_num, model, accu])\n",
    "\n",
    "#     return pd.DataFrame(out, columns=cols)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "612fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o1 = get_report_from_df(df1_test, \"blog_test\")\n",
    "# o2 = get_report_from_df(df2_test, \"CCAT50_test\")\n",
    "# o3 = get_report_from_df(df3_test, \"enron_test\")\n",
    "# o4 = get_report_from_df(df4_test, \"reddit_test\")\n",
    "# o = pd.concat([o1, o2, o3, o4], axis=0)\n",
    "# o = o.reset_index(drop=True)\n",
    "# o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a2284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ed9611",
   "metadata": {},
   "source": [
    "#### deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-01 20:37:52.092367: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746146272.103514 2470291 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746146272.107141 2470291 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746146272.117583 2470291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746146272.117594 2470291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746146272.117596 2470291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746146272.117597 2470291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-01 20:37:52.120610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def get_text_encodings(model_name, texts, max_length):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return  tokenizer(texts, truncation=True, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_length)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def get_dataset(model_name, texts,\n",
    "                max_length, labels):\n",
    "    \n",
    "    encodings = get_text_encodings(model_name, texts, \n",
    "                                   max_length)\n",
    "\n",
    "    dataset = CustomDataset(encodings, labels)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_model_and_trainer(ckpt_dir):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(ckpt_dir)\n",
    "    trainer = Trainer(model=model)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def print_classification_report(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "\n",
    "def sanity_check_av_ckpt_dir(ckpt_dir, test_set_fp=None, \n",
    "                             test_on_samples=False, samples_size=1000):\n",
    "    ckpt_dir_parent = os.path.dirname(ckpt_dir)\n",
    "\n",
    "    with open(os.path.join(ckpt_dir_parent, \"args.json\"), \"r\") as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    model_name = args[\"model_name\"]\n",
    "    max_length = args[\"max_length\"]\n",
    "    if test_set_fp is None:\n",
    "        test_set_fp = args[\"test_df_fp\"]\n",
    "\n",
    "    print(f\"Test set file path: {test_set_fp}\")\n",
    "    df = pd.read_csv(test_set_fp)\n",
    "\n",
    "    if test_on_samples:\n",
    "        df = df.sample(samples_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    labels = df[\"AA-label\"].tolist()\n",
    "    dataset = get_dataset(model_name, df[\"text\"].tolist(), \n",
    "                          max_length, labels)\n",
    "    \n",
    "    trainer = get_model_and_trainer(ckpt_dir)\n",
    "    predictions = trainer.predict(dataset)\n",
    "    y_pred = predictions.predictions.argmax(-1)\n",
    "\n",
    "    model_name = model_name.split('/')[-1]\n",
    "    prev_y_pred = df[f\"{model_name}-AA-prediction\"]\n",
    "    overlap = (y_pred == prev_y_pred).mean()\n",
    "    print(f\"Overlap: {overlap:.2f}\")\n",
    "\n",
    "    # print_classification_report(y_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c50b083a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/blog_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/longformer-base-4096/blog/checkpoint-6300\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57494785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/CCAT50_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/longformer-base-4096/CCAT50/checkpoint-620\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea89abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/enron_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/longformer-base-4096/enron/checkpoint-970\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02e4b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/reddit_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/longformer-base-4096/reddit/checkpoint-2110\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "591d2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/blog_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/ModernBERT-base/blog/checkpoint-2524\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eec99acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/CCAT50_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/ModernBERT-base/CCAT50/checkpoint-620\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ae84dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/enron_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/ModernBERT-base/enron/checkpoint-970\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1016fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/reddit_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 0.99\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/ModernBERT-base/reddit/checkpoint-2110\", \n",
    "                         test_on_samples=True, samples_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c97cd20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set file path: dataset_prepare/reddit_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "sanity_check_av_ckpt_dir(\"AA_models/ModernBERT-base/reddit/checkpoint-2110\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40d7797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'prev_y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(dataset)\n\u001b[1;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m overlap \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m \u001b[43mprev_y_pred\u001b[49m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prev_y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"AA_models/ModernBERT-base/reddit/checkpoint-2110\"\n",
    "ckpt_dir_parent = os.path.dirname(ckpt_dir)\n",
    "with open(os.path.join(ckpt_dir_parent, \"args.json\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "model_name = args[\"model_name\"]\n",
    "max_length = args[\"max_length\"]\n",
    "df = pd.read_csv(args[\"test_df_fp\"])\n",
    "df = df.sample(1000, random_state=42).reset_index(drop=True)\n",
    "labels = df[\"AA-label\"].tolist()\n",
    "dataset = get_dataset(model_name, df[\"text\"].tolist(), \n",
    "                      max_length, labels)\n",
    "trainer = get_model_and_trainer(ckpt_dir)\n",
    "predictions = trainer.predict(dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6006742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 0.99\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name.split('/')[-1]\n",
    "prev_y_pred = df[f\"{model_name}-AA-prediction\"]\n",
    "overlap = (y_pred == prev_y_pred).mean()\n",
    "print(f\"Overlap: {overlap:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e902135d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>AA-label</th>\n",
       "      <th>longformer-base-4096-AA-prediction</th>\n",
       "      <th>longformer-base-4096-AA-probabilities</th>\n",
       "      <th>ModernBERT-base-AA-prediction</th>\n",
       "      <th>ModernBERT-base-AA-probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>252998</td>\n",
       "      <td>TooManyInLitter</td>\n",
       "      <td>Go, try to enjoy yourself. Look at all the lit...</td>\n",
       "      <td>TrueAtheism</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.019077</td>\n",
       "      <td>86</td>\n",
       "      <td>0.001097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1252363</td>\n",
       "      <td>adrianmonk</td>\n",
       "      <td>the more scales you know, the easier this is g...</td>\n",
       "      <td>piano</td>\n",
       "      <td>36</td>\n",
       "      <td>63</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>295797</td>\n",
       "      <td>Philo_T_Farnsworth</td>\n",
       "      <td>If ever there was a single movie that defined ...</td>\n",
       "      <td>Physics</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1779037</td>\n",
       "      <td>avapoet</td>\n",
       "      <td>Good advice generally, but not true. Cancellin...</td>\n",
       "      <td>AskUK</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>90</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1012050</td>\n",
       "      <td>tubcat</td>\n",
       "      <td>Here's my honest opinion here. Find a jumping ...</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>98</td>\n",
       "      <td>0.014395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  ... ModernBERT-base-AA-probabilities\n",
       "157   252998  ...                         0.001097\n",
       "215  1252363  ...                         0.000527\n",
       "320   295797  ...                         0.001876\n",
       "338  1779037  ...                         0.000850\n",
       "478  1012050  ...                         0.014395\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[y_pred != prev_y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "logits = predictions.predictions  # This contains the raw logits output\n",
    "    # Convert logits to probabilities using softmax\n",
    "probabilities = softmax(torch.tensor(logits), dim=1)\n",
    "\n",
    "topk_values, topk_indices = torch.topk(probabilities, k=10, dim=1)\n",
    "\n",
    "# Convert to Python lists for further use\n",
    "top10_probs = topk_values.tolist()\n",
    "top10_labels = topk_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ef24dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"top10_probs\"] = top10_probs\n",
    "df[\"top10_labels\"] = top10_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf8f3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>AA-label</th>\n",
       "      <th>longformer-base-4096-AA-prediction</th>\n",
       "      <th>longformer-base-4096-AA-probabilities</th>\n",
       "      <th>ModernBERT-base-AA-prediction</th>\n",
       "      <th>ModernBERT-base-AA-probabilities</th>\n",
       "      <th>top10_probs</th>\n",
       "      <th>top10_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>252998</td>\n",
       "      <td>TooManyInLitter</td>\n",
       "      <td>Go, try to enjoy yourself. Look at all the lit...</td>\n",
       "      <td>TrueAtheism</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.019077</td>\n",
       "      <td>86</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>[0.18032802641391754, 0.17674608528614044, 0.1...</td>\n",
       "      <td>[30, 86, 79, 42, 64, 40, 10, 15, 52, 43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1252363</td>\n",
       "      <td>adrianmonk</td>\n",
       "      <td>the more scales you know, the easier this is g...</td>\n",
       "      <td>piano</td>\n",
       "      <td>36</td>\n",
       "      <td>63</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>[0.1878989189863205, 0.18598251044750214, 0.16...</td>\n",
       "      <td>[26, 2, 84, 59, 39, 29, 25, 64, 36, 77]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>295797</td>\n",
       "      <td>Philo_T_Farnsworth</td>\n",
       "      <td>If ever there was a single movie that defined ...</td>\n",
       "      <td>Physics</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>[0.23631545901298523, 0.23211009800434113, 0.1...</td>\n",
       "      <td>[35, 4, 81, 80, 28, 94, 87, 34, 20, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1779037</td>\n",
       "      <td>avapoet</td>\n",
       "      <td>Good advice generally, but not true. Cancellin...</td>\n",
       "      <td>AskUK</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>90</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>[0.2719070017337799, 0.2707090377807617, 0.112...</td>\n",
       "      <td>[33, 90, 49, 5, 25, 47, 42, 53, 3, 89]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1012050</td>\n",
       "      <td>tubcat</td>\n",
       "      <td>Here's my honest opinion here. Find a jumping ...</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>98</td>\n",
       "      <td>0.014395</td>\n",
       "      <td>[0.44210806488990784, 0.4293091893196106, 0.04...</td>\n",
       "      <td>[11, 98, 7, 56, 92, 1, 91, 71, 17, 15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  ...                              top10_labels\n",
       "157   252998  ...  [30, 86, 79, 42, 64, 40, 10, 15, 52, 43]\n",
       "215  1252363  ...   [26, 2, 84, 59, 39, 29, 25, 64, 36, 77]\n",
       "320   295797  ...   [35, 4, 81, 80, 28, 94, 87, 34, 20, 31]\n",
       "338  1779037  ...    [33, 90, 49, 5, 25, 47, 42, 53, 3, 89]\n",
       "478  1012050  ...    [11, 98, 7, 56, 92, 1, 91, 71, 17, 15]\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[y_pred != prev_y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15b08a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_accuracy(y_true, y_pred, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the top-k accuracy.\n",
    "    \"\"\"\n",
    "    top_k_correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] in y_pred[i][:k]:\n",
    "            top_k_correct += 1\n",
    "    return top_k_correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9404b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.68\n",
      "Top-2 accuracy: 0.79\n",
      "Top-3 accuracy: 0.82\n",
      "Top-4 accuracy: 0.85\n",
      "Top-5 accuracy: 0.87\n",
      "Top-6 accuracy: 0.89\n",
      "Top-7 accuracy: 0.91\n",
      "Top-8 accuracy: 0.92\n",
      "Top-9 accuracy: 0.93\n",
      "Top-10 accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 11):\n",
    "    top_k_accu = get_top_k_accuracy(labels, top10_labels, k=k)\n",
    "    print(f\"Top-{k} accuracy: {top_k_accu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d3193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ace3f4e",
   "metadata": {},
   "source": [
    "### Real deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b30f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def get_text_encodings(model_name, texts, max_length):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return  tokenizer(texts, truncation=True, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_length)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def get_dataset(model_name, texts,\n",
    "                max_length, labels):\n",
    "    \n",
    "    encodings = get_text_encodings(model_name, texts, \n",
    "                                   max_length)\n",
    "\n",
    "    dataset = CustomDataset(encodings, labels)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_model_and_trainer(ckpt_dir):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(ckpt_dir)\n",
    "    trainer = Trainer(model=model)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def print_classification_report(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "\n",
    "def deploy_an_AA_model(ckpt_dir, deploy_fp, \n",
    "                       text_col=\"writing\", \n",
    "                       top_k=10, overwrite=False):\n",
    "    \n",
    "    ckpt_dir_parent = os.path.dirname(ckpt_dir)\n",
    "\n",
    "    with open(os.path.join(ckpt_dir_parent, \"args.json\"), \"r\") as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    model_name = args[\"model_name\"]\n",
    "    max_length = args[\"max_length\"]\n",
    "    model_name__ = model_name.split('/')[-1]\n",
    "\n",
    "    df = pd.read_csv(deploy_fp)\n",
    "\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_col}' not found in the DataFrame.\")\n",
    "\n",
    "    df[text_col] = df[text_col].fillna(\"SOMETHING_WRONG\")\n",
    "    \n",
    "    if f\"{model_name__}-AA-top_k-predictions\" in df.columns and not overwrite:\n",
    "        print(f\"Column '{model_name__}-AA-top_k-predictions' already exists in the DataFrame. \"\n",
    "              f\"Set 'overwrite=True' to overwrite it.\")\n",
    "        return\n",
    "\n",
    "    labels = [0] * len(df)  # Dummy labels, not used in prediction\n",
    "    dataset = get_dataset(model_name, \n",
    "                          df[text_col].tolist(), \n",
    "                          max_length, labels)\n",
    "    \n",
    "    trainer = get_model_and_trainer(ckpt_dir)\n",
    "    predictions = trainer.predict(dataset)\n",
    "    logits = predictions.predictions  # This contains the raw logits output\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = softmax(torch.tensor(logits), dim=1)\n",
    "\n",
    "    topk_values, topk_indices = torch.topk(probabilities, k=top_k, dim=1)\n",
    "\n",
    "    # Convert to Python lists for further use\n",
    "    top_k_probs = topk_values.tolist()\n",
    "    top_k_preds = topk_indices.tolist()\n",
    "    df[f\"{model_name__}-AA-top_k-probabilities\"] = top_k_probs\n",
    "    df[f\"{model_name__}-AA-top_k-predictions\"] = top_k_preds\n",
    "    df.to_csv(deploy_fp, index=False)\n",
    "    print(f\"Deployment completed. Results saved to {deploy_fp}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    models = [\"AA_models/longformer-base-4096\", \n",
    "              \"AA_models/ModernBERT-base\"]\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            dir_path = os.path.join(model, dataset)\n",
    "            ckpt_dir_names = [dn for dn in os.listdir(dir_path) if dn.startswith(\"checkpoint-\")]\n",
    "            \n",
    "            if not ckpt_dir_names:\n",
    "                print(f\"No checkpoints found in {dir_path}.\")\n",
    "                continue\n",
    "            \n",
    "            # select the latest checkpoint\n",
    "            ckpt_dir_names.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "            ckpt_dir = os.path.join(dir_path, ckpt_dir_names[-1])\n",
    "\n",
    "            for setting in [1, 2, 3, 4, 5]:\n",
    "                dataset_dir = os.path.join(\"LLM_writing\", f\"Setting{setting}\", dataset)\n",
    "                if not os.path.exists(dataset_dir):\n",
    "                    print(f\"Directory for Setting {setting} and dataset {dataset} does not exist.\")\n",
    "                    continue\n",
    "            \n",
    "                prompt_fp = os.path.join(dataset_dir, \"prompts.csv\")\n",
    "                if not os.path.exists(prompt_fp):\n",
    "                    print(f\"Prompts file not found in {dataset_dir}.\")\n",
    "                    continue\n",
    "                \n",
    "                df_prompts = pd.read_csv(prompt_fp)\n",
    "\n",
    "                llm_fps = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) \n",
    "                        if f.endswith(\".csv\") and f != \"prompts.csv\"]\n",
    "                \n",
    "                for llm_fp in llm_fps:\n",
    "                    llm_df = pd.read_csv(llm_fp)\n",
    "\n",
    "                    if len(df_prompts) != len(llm_df):\n",
    "                        print(f\"Length mismatch between prompts and LLM-generated writing for {llm_fp}.\")\n",
    "                        continue\n",
    "\n",
    "                    deploy_an_AA_model(ckpt_dir, llm_fp,\n",
    "                                    text_col=\"writing\", \n",
    "                                    top_k=10, overwrite=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa02a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_dir = \"AA_models/longformer-base-4096/blog/checkpoint-6300\"\n",
    "fp = \"LLM_writing/Setting1/blog/gemini-2.0-flash.csv\"\n",
    "deploy_an_AA_model(ckpt_dir, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3e28bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-630', 'checkpoint-3300']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([\"checkpoint-630\", \"checkpoint-3300\"], key =lambda x: int(x.split(\"-\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17eabf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75febe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b28b38fe",
   "metadata": {},
   "source": [
    "### AV models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090af3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def get_text_encodings(model_name, texts1, texts2, max_length):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer(texts1, texts2, \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\", \n",
    "                     max_length=max_length)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) \n",
    "                for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def get_dataset(model_name, texts1, texts2,\n",
    "                max_length, labels):\n",
    "    \n",
    "    encodings = get_text_encodings(model_name, \n",
    "                                   texts1, texts2, \n",
    "                                   max_length)\n",
    "\n",
    "    dataset = CustomDataset(encodings, labels)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_model_and_trainer(model_load_file):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_load_file)\n",
    "    trainer = Trainer(model=model)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def deploy_an_AV_model(ckpt_dir, \n",
    "                       deploy_fp1, \n",
    "                       deploy_fp2,\n",
    "                       text_col1,\n",
    "                       text_col2,\n",
    "                       overwrite=False):\n",
    "    ckpt_dir_parent = os.path.dirname(ckpt_dir)\n",
    "\n",
    "    with open(os.path.join(ckpt_dir_parent, \"args.json\"), \"r\") as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    model_name = args[\"model_name\"]\n",
    "    max_length = args[\"max_length\"]\n",
    "    \n",
    "    df1 = pd.read_csv(deploy_fp1)[:1000]\n",
    "    df2 = pd.read_csv(deploy_fp2)[:1000]\n",
    "\n",
    "    assert df1.shape[0] == df2.shape[0], \\\n",
    "        f\"DataFrames must have the same number of rows. \" \\\n",
    "        f\"Got {df1.shape[0]} and {df2.shape[0]} rows.\"\n",
    "\n",
    "    model_name__ = model_name.split('/')[-1]\n",
    "    if f\"{model_name__}-prediction\" in df2.columns and not overwrite:\n",
    "        print(f\"Column '{model_name__}-prediction' already exists in the DataFrame. \"\n",
    "              f\"Set 'overwrite=True' to overwrite it.\")\n",
    "        return\n",
    "\n",
    "    if text_col1 not in df1.columns:\n",
    "        raise ValueError(f\"Column '{text_col1}' not found in the DataFrame.\")\n",
    "    if text_col2 not in df2.columns:\n",
    "        raise ValueError(f\"Column '{text_col2}' not found in the DataFrame.\")\n",
    "    \n",
    "    df1[text_col1] = df1[text_col1].fillna(\"SOMETHING_WRONG\")\n",
    "    df2[text_col2] = df2[text_col2].fillna(\"SOMETHING_WRONG\")\n",
    "\n",
    "\n",
    "    labels = [0] * len(df1)  # Dummy labels, not used in prediction\n",
    "    dataset = get_dataset(model_name, df1[text_col1].tolist(), \n",
    "                          df2[text_col2].tolist(), max_length, labels)\n",
    "    \n",
    "    trainer = get_model_and_trainer(ckpt_dir)\n",
    "    predictions = trainer.predict(dataset)\n",
    "    y_pred = predictions.predictions.argmax(-1)\n",
    "    logits = predictions.predictions  # This contains the raw logits output\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = softmax(torch.tensor(logits), dim=1).tolist()\n",
    "\n",
    "    df2[f\"{model_name__}-prediction\"] = y_pred\n",
    "    df2[f\"{model_name__}-probabilities\"] = probabilities\n",
    "    df2.to_csv(deploy_fp2, index=False)\n",
    "    print(f\"Deployment completed. Results saved to {deploy_fp2}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    models = [\"AV_models/longformer-base-4096\", \n",
    "              \"AV_models/ModernBERT-base\"]\n",
    "    datasets = [\"CCAT50\", \"enron\", \"reddit\", \"blog\"]\n",
    "\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            dir_path = os.path.join(model, dataset)\n",
    "            ckpt_dir_names = [dn for dn in os.listdir(dir_path) if dn.startswith(\"checkpoint-\")]\n",
    "            \n",
    "            if not ckpt_dir_names:\n",
    "                print(f\"No checkpoints found in {dir_path}.\")\n",
    "                continue\n",
    "            \n",
    "            # select the latest checkpoint\n",
    "            ckpt_dir_names.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "            ckpt_dir = os.path.join(dir_path, ckpt_dir_names[-1])\n",
    "\n",
    "            for setting in [1, 2, 3, 4, 5]:\n",
    "                dataset_dir = os.path.join(\"LLM_writing\", f\"Setting{setting}\", dataset)\n",
    "                if not os.path.exists(dataset_dir):\n",
    "                    print(f\"Directory for Setting {setting} and dataset {dataset} does not exist.\")\n",
    "                    continue\n",
    "                    \n",
    "                prompt_fp = os.path.join(dataset_dir, \"prompts.csv\")\n",
    "                if not os.path.exists(prompt_fp):\n",
    "                    print(f\"Prompts file not found in {dataset_dir}.\")\n",
    "                    continue\n",
    "                df_prompts = pd.read_csv(prompt_fp)\n",
    "                \n",
    "                llm_fps = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) \n",
    "                        if f.endswith(\".csv\") and f != \"prompts.csv\"]\n",
    "                \n",
    "                for llm_fp in llm_fps:\n",
    "                    llm_df = pd.read_csv(llm_fp)\n",
    "\n",
    "                    if len(df_prompts) != len(llm_df):\n",
    "                        print(f\"Length mismatch between prompts and LLM-generated writing for {llm_fp}.\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"===> Deploying model {model} on {llm_fp}\")\n",
    "                    deploy_an_AV_model(ckpt_dir, prompt_fp, \n",
    "                                       llm_df, text_col1=\"text\",\n",
    "                                       text_col2=\"writing\",\n",
    "                                       overwrite=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4386557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbba675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faba20b9",
   "metadata": {},
   "source": [
    "### Style Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c90d5909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA_models\t\t\t      LLM_writing\n",
      "AV_models\t\t\t      notebooks\n",
      "create_stylometry_features.py\t      README.md\n",
      "create_summaries_for_eval_samples.py  requirements.txt\n",
      "create_summaries.sh\t\t      scripts\n",
      "dataset_prepare\t\t\t      Style_features_LLM\n",
      "fileStructure.png\t\t      train_AA_classifiers.sh\n",
      "generate_llm_writing.py\t\t      train_and_eval_an_AA_model.py\n",
      "generate_llm_writing.sh\t\t      train_and_eval_an_AV_model.py\n",
      "LIWC2007_English100131.dic\t      train_AV_classifiers.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce955e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writing</th>\n",
       "      <th>liwc_features</th>\n",
       "      <th>writeprint_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so, alltel. :) monthly visits to see the hotti...</td>\n",
       "      <td>{'liwc_funct_frac': 0.3575, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.0584, 'letter_b': 0.0173, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IT Jobs Moving Overseas\\n\\nAccording to a Gart...</td>\n",
       "      <td>{'liwc_funct_frac': 0.4196, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.0378, 'letter_b': 0.0198, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, this nightclub fire thing. Awful, right? J...</td>\n",
       "      <td>{'liwc_funct_frac': 0.409, 'liwc_pronoun_frac'...</td>\n",
       "      <td>{'letter_a': 0.0644, 'letter_b': 0.0145, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugh, conservatives. Seriously, what IS that ab...</td>\n",
       "      <td>{'liwc_funct_frac': 0.3946, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.054, 'letter_b': 0.0154, 'lette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beach trip! yay! the weather was so nice, and ...</td>\n",
       "      <td>{'liwc_funct_frac': 0.4516, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.062, 'letter_b': 0.0113, 'lette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>So, Rolling Stone just dropped their top 500 a...</td>\n",
       "      <td>{'liwc_funct_frac': 0.3984, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.0583, 'letter_b': 0.0168, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25221</th>\n",
       "      <td>Reflections on Doing All to the Glory of God\\n...</td>\n",
       "      <td>{'liwc_funct_frac': 0.5145, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.0499, 'letter_b': 0.0138, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25222</th>\n",
       "      <td>Hey Gals! Things have been so crazy busy here ...</td>\n",
       "      <td>{'liwc_funct_frac': 0.465, 'liwc_pronoun_frac'...</td>\n",
       "      <td>{'letter_a': 0.0547, 'letter_b': 0.0118, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25223</th>\n",
       "      <td>Eh, whatever about the World Series. I used to...</td>\n",
       "      <td>{'liwc_funct_frac': 0.4655, 'liwc_pronoun_frac...</td>\n",
       "      <td>{'letter_a': 0.0638, 'letter_b': 0.0213, 'lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25224</th>\n",
       "      <td>So, i got this infection. Nasty stuff. Right i...</td>\n",
       "      <td>{'liwc_funct_frac': 0.481, 'liwc_pronoun_frac'...</td>\n",
       "      <td>{'letter_a': 0.0462, 'letter_b': 0.0055, 'lett...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25225 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 writing  ...                                writeprint_features\n",
       "0      so, alltel. :) monthly visits to see the hotti...  ...  {'letter_a': 0.0584, 'letter_b': 0.0173, 'lett...\n",
       "1      IT Jobs Moving Overseas\\n\\nAccording to a Gart...  ...  {'letter_a': 0.0378, 'letter_b': 0.0198, 'lett...\n",
       "2      So, this nightclub fire thing. Awful, right? J...  ...  {'letter_a': 0.0644, 'letter_b': 0.0145, 'lett...\n",
       "3      Ugh, conservatives. Seriously, what IS that ab...  ...  {'letter_a': 0.054, 'letter_b': 0.0154, 'lette...\n",
       "4      beach trip! yay! the weather was so nice, and ...  ...  {'letter_a': 0.062, 'letter_b': 0.0113, 'lette...\n",
       "...                                                  ...  ...                                                ...\n",
       "25220  So, Rolling Stone just dropped their top 500 a...  ...  {'letter_a': 0.0583, 'letter_b': 0.0168, 'lett...\n",
       "25221  Reflections on Doing All to the Glory of God\\n...  ...  {'letter_a': 0.0499, 'letter_b': 0.0138, 'lett...\n",
       "25222  Hey Gals! Things have been so crazy busy here ...  ...  {'letter_a': 0.0547, 'letter_b': 0.0118, 'lett...\n",
       "25223  Eh, whatever about the World Series. I used to...  ...  {'letter_a': 0.0638, 'letter_b': 0.0213, 'lett...\n",
       "25224  So, i got this infection. Nasty stuff. Right i...  ...  {'letter_a': 0.0462, 'letter_b': 0.0055, 'lett...\n",
       "\n",
       "[25225 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Style_features_LLM/Setting1/blog/gemini-2.0-flash_features.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c7097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e15232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4d07b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['writing', 'longformer-base-4096-AA-top_k-probabilities',\n",
       "       'longformer-base-4096-AA-top_k-predictions',\n",
       "       'ModernBERT-base-AA-top_k-probabilities',\n",
       "       'ModernBERT-base-AA-top_k-predictions', 'ModernBERT-base-prediction',\n",
       "       'ModernBERT-base-probabilities', 'ModernBERT-base-AV-prediction',\n",
       "       'ModernBERT-base-AV-probabilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"LLM_writing/Setting1/blog/gpt-4o-2024-08-06.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d9dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
