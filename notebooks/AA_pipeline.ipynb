{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d7dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1710d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import evaluate  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train and evaluate an AA model.\")\n",
    "    parser.add_argument(\"--training_df_fp\", type=str, required=True, help=\"Filepath for the training dataset\")\n",
    "    parser.add_argument(\"--test_df_fp\", type=str, required=True, help=\"Filepath for the test dataset\")\n",
    "\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"allenai/longformer-base-4096\", help=\"Name of the model to be used\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=2048, help=\"Maximum length of the input sequences\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=10, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=8, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=16, help=\"Batch size for evaluation\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500, help=\"Number of warmup steps for learning rate scheduler\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for optimizer\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5, help=\"Learning rate for optimizer\")\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Logging steps\")\n",
    "    parser.add_argument(\"--evaluation_strategy\", type=str, default=\"epoch\", help=\"Evaluation strategy\")\n",
    "    parser.add_argument(\"--load_best_model_at_end\", type=str, default=\"True\", help=\"Load the best model at the end of training\")\n",
    "    parser.add_argument(\"--fp16\", type=str, default=\"True\", help=\"Use mixed precision training\")\n",
    "    parser.add_argument(\"--save_total_limit\", type=int, default=1, help=\"Limit the total amount of checkpoints\")\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=\"True\", help=\"Resume training from checkpoint\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def get_author_map(df, author_col=\"author\"):\n",
    "    author_map = {author: i for i, author in \n",
    "    enumerate(df[author_col].unique())}\n",
    "    return author_map\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def bool_str_to_bool(value):\n",
    "    return value.lower() in ('true', '1', 'yes', 'y', 't')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    f1_score = evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_score.add_batch(predictions=predictions, references=labels)\n",
    "    return f1_score.compute(average=\"weighted\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    # Check if the dataset names match\n",
    "    # between training and evaluation datasets\n",
    "    dataset = args.training_df_fp.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "    dataset_ = args.test_df_fp.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "    assert dataset == dataset_, f\"Dataset name mismatch: {dataset} != {dataset_}\"\n",
    "\n",
    "    # Load the training and test datasets\n",
    "    df = pd.read_csv(args.training_df_fp)\n",
    "    test_df = pd.read_csv(args.test_df_fp)\n",
    "\n",
    "    # Get the author map\n",
    "    if \"label\" not in df.columns:\n",
    "        author_map = get_author_map(df)\n",
    "        df[\"label\"] = df[\"author\"].map(author_map)\n",
    "        test_df[\"label\"] = test_df[\"author\"].map(author_map)\n",
    "        df.to_csv(args.training_df_fp, index=False)\n",
    "        test_df.to_csv(args.test_df_fp, index=False)\n",
    "        print(f\"Appended author labels to {args.training_df_fp} and {args.test_df_fp}\") \n",
    "    \n",
    "    # Split the training data into train and validation sets\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, \n",
    "                                          random_state=42, \n",
    "                                          stratify=df[\"label\"])\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "    # Load the tokenizer and tokenize the datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    train_encodings = tokenizer(list(train_df['text']), \n",
    "                                truncation=True, padding=\"max_length\",\n",
    "                                max_length=args.max_length)\n",
    "    valid_encodings = tokenizer(list(valid_df['text']), \n",
    "                                truncation=True, padding=\"max_length\",\n",
    "                                max_length=args.max_length)\n",
    "    test_encodings = tokenizer(list(test_df['text']),\n",
    "                               truncation=True, padding=\"max_length\",\n",
    "                               max_length=args.max_length)\n",
    "    train_dataset = CustomDataset(train_encodings, train_df['label'])\n",
    "    valid_dataset = CustomDataset(valid_encodings, valid_df['label'])\n",
    "    test_dataset = CustomDataset(test_encodings, test_df['label'])\n",
    "\n",
    "    # Load the model and train it\n",
    "    model_output_dir = f\"./AA_models/{args.model_name.split('/')[-1]}/\" + dataset\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, device_map=\"auto\",\n",
    "                                                               num_labels=len(df[\"label\"].unique()))\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,  # output directory\n",
    "        fp16=bool_str_to_bool(args.fp16),  # Use mixed precision training\n",
    "        num_train_epochs=args.num_train_epochs,  # total # of training epochs\n",
    "        per_device_train_batch_size=args.train_batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=args.eval_batch_size,  # batch size for evaluation\n",
    "        # number of updates steps to accumulate before performing a backward/update pass\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,  \n",
    "        warmup_steps=args.warmup_steps,  # Number of warmup steps for learning rate scheduler\n",
    "        weight_decay=args.weight_decay,  # Strength of weight decay\n",
    "        learning_rate=args.learning_rate,  # Initial learning rate\n",
    "        save_total_limit=args.save_total_limit,  # Limit the total amount of checkpoints\n",
    "        logging_steps=args.logging_steps,  # Log every X updates steps\n",
    "        evaluation_strategy=args.evaluation_strategy,  # evaluation strategy to adopt during training\n",
    "        save_strategy=args.evaluation_strategy,  # save strategy to adopt during training\n",
    "        load_best_model_at_end=args.load_best_model_at_end,  # load the best model when finished training\n",
    "        metric_for_best_model=\"eval_loss\",  # use f1 score to compare models\n",
    "        greater_is_better=False,  # f1 score should be greater\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    resume_from_checkpoint = bool_str_to_bool(args.resume_from_checkpoint)\n",
    "    if any([\"checkpoint\" in file for file in os.listdir(model_output_dir)]) and resume_from_checkpoint:\n",
    "        print(\"Resuming from the latest checkpoint\")\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = predictions.predictions.argmax(-1)\n",
    "\n",
    "    y_test = test_df.label.tolist()\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    logits = predictions.predictions  # This contains the raw logits output\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = softmax(torch.tensor(logits), dim=1).tolist()\n",
    "\n",
    "    model_name = args.model_name.split('/')[-1]\n",
    "    test_df[f\"{model_name}-prediction\"]=y_pred\n",
    "    test_df[f\"{model_name}-probabilities\"] = [prob[1] for prob in probabilities]\n",
    "    test_df.to_csv(args.test_df_fp, index=False)\n",
    "    print(f\"Predictions and probabilities by {args.model_name} saved to {args.test_df_fp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13df8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638e534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79371aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201da49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_map(df, author_col=\"author\"):\n",
    "    author_map = {author: i for i, author in \n",
    "    enumerate(df[author_col].unique())}\n",
    "    return author_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"dataset_prepare/blog_train.csv\")\n",
    "test_df = pd.read_csv(\"dataset_prepare/blog_test.csv\")\n",
    "author_map = get_author_map(df, author_col=\"author\")\n",
    "df[\"label\"] = df[\"author\"].map(author_map)\n",
    "test_df[\"label\"] = test_df[\"author\"].map(author_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af6d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0dbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2174adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "max_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=len(author_map), \n",
    "                                                           device_map=\"auto\")\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "train_encodings = tokenizer(list(train_df['text']),truncation=True, padding=\"max_length\",max_length=max_length)\n",
    "valid_encodings = tokenizer(list(valid_df['text']), truncation=True, padding=\"max_length\",max_length=max_length)\n",
    "test_encodings = tokenizer(list(test_df['text']),truncation=True, padding=\"max_length\",max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb5a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_encodings, train_df['label'])\n",
    "valid_dataset = CustomDataset(valid_encodings, valid_df['label'])\n",
    "test_dataset = CustomDataset(test_encodings, test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98d65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90a504b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/316 00:34 < 59:22, 0.09 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.806200</td>\n",
       "      <td>3.822578</td>\n",
       "      <td>0.166283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='434' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 04:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 48\u001b[0m\n\u001b[1;32m     38\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     40\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)],  \u001b[38;5;66;03m# Early stopping patience\u001b[39;00m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/trainer.py:2620\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/trainer.py:3093\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3091\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3093\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3094\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/trainer.py:3047\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3047\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3048\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/trainer.py:4136\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4133\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4135\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4136\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4146\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/transformers/trainer.py:4352\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4350\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m   4351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4352\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4354\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/accelerate/accelerator.py:2713\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/accelerate/utils/operations.py:408\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    410\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/accelerate/utils/operations.py:678\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    675\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/PW/lib/python3.12/site-packages/accelerate/utils/operations.py:658\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    655\u001b[0m     dim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    659\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate  # Make sure to import this\n",
    "def compute_metrics(eval_pred):\n",
    "    f1_score = evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_score.add_batch(predictions=predictions, references=labels)\n",
    "    return f1_score.compute(average=\"weighted\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "dataset = \"blog\"\n",
    "model_output_dir = f\"./AA_models/{model_name.split('/')[-1]}/\" + dataset\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,  # output directory\n",
    "    num_train_epochs=1,  # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=100,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    learning_rate=2e-5,  # learning rate\n",
    "    save_total_limit=1,  # limit the total amount of checkpoints, delete older checkpoints\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",  # evaluate at the end of each epoch\n",
    "    save_strategy=\"steps\",  # save model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # metric to track for early stopping\n",
    "    greater_is_better=False,  # validation loss should decrease\n",
    ")\n",
    "\n",
    "# Create the Trainer object with Early Stopping Callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Early stopping patience\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dbb644c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       218\n",
      "           1       0.00      0.00      0.00       216\n",
      "           2       0.00      0.00      0.00       179\n",
      "           3       1.00      0.01      0.02       221\n",
      "           4       0.00      0.00      0.00       262\n",
      "           5       0.50      0.01      0.01       273\n",
      "           6       0.05      0.20      0.09       308\n",
      "           7       0.14      0.11      0.12       170\n",
      "           8       0.20      0.01      0.02       268\n",
      "           9       0.12      0.54      0.20       375\n",
      "          10       0.68      0.57      0.62       261\n",
      "          11       0.09      0.42      0.14       406\n",
      "          12       0.30      0.02      0.04       304\n",
      "          13       0.27      0.01      0.02       256\n",
      "          14       0.00      0.00      0.00       197\n",
      "          15       0.16      0.02      0.03       322\n",
      "          16       0.50      0.68      0.58       204\n",
      "          17       0.00      0.00      0.00       207\n",
      "          18       0.42      0.37      0.39       306\n",
      "          19       0.15      0.48      0.23       442\n",
      "          20       0.09      0.13      0.11       404\n",
      "          21       0.30      0.19      0.23       264\n",
      "          22       0.41      0.11      0.17       222\n",
      "          23       0.00      0.00      0.00       227\n",
      "          24       0.00      0.00      0.00       184\n",
      "          25       0.49      0.59      0.54       202\n",
      "          26       0.13      0.10      0.12       250\n",
      "          27       0.40      0.96      0.57       272\n",
      "          28       0.67      0.63      0.65       273\n",
      "          29       0.19      0.59      0.29       437\n",
      "          30       0.50      0.67      0.57       384\n",
      "          31       0.14      0.13      0.13       277\n",
      "          32       0.15      0.67      0.24       510\n",
      "          33       0.56      0.02      0.05       204\n",
      "          34       0.39      0.40      0.39       168\n",
      "          35       0.37      0.15      0.21       338\n",
      "          36       0.00      0.00      0.00       179\n",
      "          37       0.00      0.00      0.00       263\n",
      "          38       0.27      0.08      0.12       247\n",
      "          39       0.67      0.01      0.02       212\n",
      "          40       0.00      0.00      0.00       174\n",
      "          41       0.09      0.50      0.16       384\n",
      "          42       0.00      0.00      0.00       191\n",
      "          43       0.31      0.02      0.03       276\n",
      "          44       0.21      0.05      0.08       195\n",
      "          45       0.76      0.81      0.78       298\n",
      "          46       0.25      0.00      0.01       248\n",
      "          47       0.00      0.00      0.00       197\n",
      "          48       0.00      0.00      0.00       231\n",
      "          49       0.06      0.41      0.10       216\n",
      "          50       0.63      0.36      0.46       381\n",
      "          51       0.20      0.01      0.02       322\n",
      "          52       0.82      0.05      0.09       198\n",
      "          53       0.00      0.00      0.00       230\n",
      "          54       0.36      0.97      0.52       506\n",
      "          55       1.00      0.00      0.01       303\n",
      "          56       0.16      0.17      0.16       358\n",
      "          57       0.00      0.00      0.00       217\n",
      "          58       0.10      0.20      0.13       395\n",
      "          59       0.00      0.00      0.00       212\n",
      "          60       0.00      0.00      0.00       186\n",
      "          61       0.00      0.00      0.00       213\n",
      "          62       0.94      0.17      0.29       177\n",
      "          63       0.00      0.00      0.00       219\n",
      "          64       0.10      0.01      0.01       259\n",
      "          65       0.25      0.00      0.01       213\n",
      "          66       0.59      0.39      0.47       193\n",
      "          67       0.40      0.01      0.02       186\n",
      "          68       0.00      0.00      0.00       220\n",
      "          69       0.18      0.37      0.24       383\n",
      "          70       0.00      0.00      0.00       224\n",
      "          71       0.69      0.83      0.75       375\n",
      "          72       0.39      0.93      0.55       388\n",
      "          73       0.00      0.00      0.00       213\n",
      "          74       0.00      0.00      0.00       166\n",
      "          75       0.00      0.00      0.00       185\n",
      "          76       0.00      0.00      0.00       212\n",
      "          77       0.00      0.00      0.00       191\n",
      "          78       0.27      0.46      0.34       378\n",
      "          79       0.61      0.12      0.20       181\n",
      "          80       0.17      0.01      0.01       175\n",
      "          81       0.00      0.00      0.00       176\n",
      "          82       0.00      0.00      0.00       208\n",
      "          83       0.00      0.00      0.00       171\n",
      "          84       0.00      0.00      0.00       191\n",
      "          85       0.00      0.00      0.00       216\n",
      "          86       0.13      0.29      0.18       235\n",
      "          87       0.37      0.17      0.24       178\n",
      "          88       0.00      0.00      0.00       189\n",
      "          89       0.25      0.26      0.26       276\n",
      "          90       0.63      0.26      0.37       225\n",
      "          91       0.76      0.76      0.76       243\n",
      "          92       0.00      0.00      0.00       244\n",
      "          93       1.00      0.01      0.02       168\n",
      "          94       0.00      0.00      0.00       210\n",
      "          95       0.00      0.00      0.00       173\n",
      "          96       0.97      0.16      0.27       187\n",
      "          97       0.00      0.00      0.00       168\n",
      "          98       0.00      0.00      0.00       211\n",
      "          99       0.00      0.00      0.00       175\n",
      "\n",
      "    accuracy                           0.23     25225\n",
      "   macro avg       0.24      0.18      0.14     25225\n",
      "weighted avg       0.25      0.23      0.17     25225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jack/anaconda3/envs/PW/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "\n",
    "y_test = test_df.label.tolist()\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "logits = predictions.predictions  # This contains the raw logits output\n",
    "# Convert logits to probabilities using softmax\n",
    "probabilities = softmax(torch.tensor(logits), dim=1).tolist()\n",
    "test_df[\"prediction\"] = y_pred\n",
    "test_df[\"probabilities\"] = [prob[1] for prob in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c640f3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "      <th>training sample indices</th>\n",
       "      <th>prompt</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>664485</td>\n",
       "      <td>so in comes the sexalicious alltel guy ...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>25</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>05,April,2003</td>\n",
       "      <td>The writer describes their monthly visits to t...</td>\n",
       "      <td>16746,6286,12122,9723,15564</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>94</td>\n",
       "      <td>9</td>\n",
       "      <td>0.008105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>449628</td>\n",
       "      <td>urlLink 500,000 U.S. IT Jobs Projecte...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>Aries</td>\n",
       "      <td>21,July,2003</td>\n",
       "      <td>According to a Gartner, Inc. report cited by I...</td>\n",
       "      <td>16385,12625,23405,18286,7342</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>0.006858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1651222</td>\n",
       "      <td>Yo and hello, y'all,  Plez here. Hm...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>23,February,2003</td>\n",
       "      <td>Plez reflects on a recent nightclub fire, expr...</td>\n",
       "      <td>18698,4838,19638,17138,14149</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.013973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1784456</td>\n",
       "      <td>I WILL STAND IN THEIR WAY! Ge...</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,October,2003</td>\n",
       "      <td>The speaker expresses strong opposition to the...</td>\n",
       "      <td>8703,10531,445,3840,20824</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0.010807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180519</td>\n",
       "      <td>Well! long time no post. Lots of things...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>29,May,2001</td>\n",
       "      <td>The writer describes a recent enjoyable trip t...</td>\n",
       "      <td>11103,17517,25203,11937,17823</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>92</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>585884</td>\n",
       "      <td>Hizzallar! What!  Man Horkins no questi...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>The writer criticizes a top 500 music list for...</td>\n",
       "      <td>1245,17881,12049,23407,22690</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>75</td>\n",
       "      <td>54</td>\n",
       "      <td>0.014114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25221</th>\n",
       "      <td>942828</td>\n",
       "      <td>On July 2, 1871  Venerable J...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>02,July,2004</td>\n",
       "      <td>In his July 2, 1871 sermon, Venerable John Hen...</td>\n",
       "      <td>5688,25093,4891,4433,13449</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>0.004994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25222</th>\n",
       "      <td>1417798</td>\n",
       "      <td>Hey Gals! It's gonna take me ...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>17,August,2003</td>\n",
       "      <td>The writer is busy with work while their mom i...</td>\n",
       "      <td>3751,2592,9746,7366,7875</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>0.010120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25223</th>\n",
       "      <td>1107146</td>\n",
       "      <td>To tell you the truth, and I ...</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Libra</td>\n",
       "      <td>21,October,2003</td>\n",
       "      <td>The writer expresses indifference about the ou...</td>\n",
       "      <td>17990,14594,13131,4845,21511</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>20</td>\n",
       "      <td>58</td>\n",
       "      <td>0.010056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25224</th>\n",
       "      <td>576311</td>\n",
       "      <td>mm, yeah, so once of the incisions is i...</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>The writer developed an infection at one of th...</td>\n",
       "      <td>6876,9717,2814,5945,12084</td>\n",
       "      <td>Given the following summary, your task is to g...</td>\n",
       "      <td>52</td>\n",
       "      <td>86</td>\n",
       "      <td>0.008183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25225 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                               text    topic  \\\n",
       "0       664485         so in comes the sexalicious alltel guy ...   indUnk   \n",
       "1       449628           urlLink 500,000 U.S. IT Jobs Projecte...   indUnk   \n",
       "2      1651222             Yo and hello, y'all,  Plez here. Hm...   indUnk   \n",
       "3      1784456                   I WILL STAND IN THEIR WAY! Ge...  Student   \n",
       "4       180519         Well! long time no post. Lots of things...   indUnk   \n",
       "...        ...                                                ...      ...   \n",
       "25220   585884         Hizzallar! What!  Man Horkins no questi...   indUnk   \n",
       "25221   942828                    On July 2, 1871  Venerable J...   indUnk   \n",
       "25222  1417798                   Hey Gals! It's gonna take me ...   indUnk   \n",
       "25223  1107146                   To tell you the truth, and I ...  Student   \n",
       "25224   576311         mm, yeah, so once of the incisions is i...   indUnk   \n",
       "\n",
       "       gender  age         sign              date  \\\n",
       "0        male   25       Taurus     05,April,2003   \n",
       "1        male   34        Aries      21,July,2003   \n",
       "2      female   38        Virgo  23,February,2003   \n",
       "3      female   16     Aquarius   09,October,2003   \n",
       "4      female   23       Cancer       29,May,2001   \n",
       "...       ...  ...          ...               ...   \n",
       "25220    male   17  Sagittarius    02,August,2004   \n",
       "25221  female   34       Cancer      02,July,2004   \n",
       "25222  female   35      Scorpio    17,August,2003   \n",
       "25223  female   16        Libra   21,October,2003   \n",
       "25224  female   34    Capricorn    02,August,2004   \n",
       "\n",
       "                                                 summary  \\\n",
       "0      The writer describes their monthly visits to t...   \n",
       "1      According to a Gartner, Inc. report cited by I...   \n",
       "2      Plez reflects on a recent nightclub fire, expr...   \n",
       "3      The speaker expresses strong opposition to the...   \n",
       "4      The writer describes a recent enjoyable trip t...   \n",
       "...                                                  ...   \n",
       "25220  The writer criticizes a top 500 music list for...   \n",
       "25221  In his July 2, 1871 sermon, Venerable John Hen...   \n",
       "25222  The writer is busy with work while their mom i...   \n",
       "25223  The writer expresses indifference about the ou...   \n",
       "25224  The writer developed an infection at one of th...   \n",
       "\n",
       "             training sample indices  \\\n",
       "0        16746,6286,12122,9723,15564   \n",
       "1       16385,12625,23405,18286,7342   \n",
       "2       18698,4838,19638,17138,14149   \n",
       "3          8703,10531,445,3840,20824   \n",
       "4      11103,17517,25203,11937,17823   \n",
       "...                              ...   \n",
       "25220   1245,17881,12049,23407,22690   \n",
       "25221     5688,25093,4891,4433,13449   \n",
       "25222       3751,2592,9746,7366,7875   \n",
       "25223   17990,14594,13131,4845,21511   \n",
       "25224      6876,9717,2814,5945,12084   \n",
       "\n",
       "                                                  prompt  label  prediction  \\\n",
       "0      Given the following summary, your task is to g...     94           9   \n",
       "1      Given the following summary, your task is to g...     19          27   \n",
       "2      Given the following summary, your task is to g...     61          11   \n",
       "3      Given the following summary, your task is to g...     29          11   \n",
       "4      Given the following summary, your task is to g...     92           6   \n",
       "...                                                  ...    ...         ...   \n",
       "25220  Given the following summary, your task is to g...     75          54   \n",
       "25221  Given the following summary, your task is to g...     72          72   \n",
       "25222  Given the following summary, your task is to g...     78          78   \n",
       "25223  Given the following summary, your task is to g...     20          58   \n",
       "25224  Given the following summary, your task is to g...     52          86   \n",
       "\n",
       "       probabilities  \n",
       "0           0.008105  \n",
       "1           0.006858  \n",
       "2           0.013973  \n",
       "3           0.010807  \n",
       "4           0.009998  \n",
       "...              ...  \n",
       "25220       0.014114  \n",
       "25221       0.004994  \n",
       "25222       0.010120  \n",
       "25223       0.010056  \n",
       "25224       0.008183  \n",
       "\n",
       "[25225 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609544b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
